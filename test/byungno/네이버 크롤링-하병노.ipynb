{"cells":[{"cell_type":"markdown","source":["---\n","# 크롤링 함수화"],"metadata":{}},{"cell_type":"code","execution_count":1,"source":["import requests\r\n","from bs4 import BeautifulSoup\r\n","import math\r\n","import re\r\n","import pandas as pd\r\n","import os\r\n","import time\r\n","from tqdm import tqdm\r\n","from tqdm import trange\r\n","from sys import stdout\r\n","\r\n","def crawling_article(key_word):\r\n","    Url = 'https://search.naver.com/search.naver?'\r\n","    ds = ['2020.01.01', '2021.01.01']\r\n","    #ds = ['2020.12.20', '2021.11.15']\r\n","    de = ['2020.12.31', '2021.12.06']  # 두번째 날은 어제로 설정\r\n","\r\n","    params = {\r\n","        \"where\" : 'news',\r\n","        \"sm\": 'tab_pge',\r\n","        \"query\": key_word,  # ★인코딩하지말고 넣어라★\r\n","        \"sort\": '0',\r\n","        \"photo\": '0',\r\n","        \"field\": '0',\r\n","        \"pd\": '3',\r\n","        \"ds\": '2020.01.01',\r\n","        \"de\" : '2020.12.31',\r\n","        \"cluster_rank\": '10',\r\n","        \"mynews\" : '1',\r\n","        \"office_type\": '0',\r\n","        \"office_section_code\": '0',\r\n","        \"news_office_checked\" : '1032',\r\n","        \"nso\" : r'so:r,p:from20210101to20211130,a:all',\r\n","        \"start\" : '5000'\r\n","    }\r\n","\r\n","    # 경향신문, 국민일보, 동아일보, 한겨례, 매일일보, 조선일보\r\n","    news_code = ['1032', '1005', '1020', '1028', '2385', '1023']\r\n","\r\n","    # 크롤링 할 url 저장\r\n","    urls = []\r\n","    \r\n","    page = 1\r\n","    \r\n","    for i in range(2):\r\n","        # 날짜 설정\r\n","        params['ds'] = ds[i]\r\n","        params['de'] = de[i]\r\n","        \r\n","        for code in news_code:\r\n","            params['news_office_checked'] = code\r\n","            raw = requests.get(Url, headers={'User-Agent': 'Mozilla/5.0'}, params=params)\r\n","\r\n","            dom = BeautifulSoup(raw.text, \"html.parser\")\r\n","\r\n","            try:\r\n","                a_tags = dom.select('#main_pack > div.api_sc_page_wrap > div > div > a')\r\n","                last_page = int(a_tags[-1].text.strip())\r\n","            except:\r\n","                continue\r\n","\r\n","            for start in range(1, last_page * 10, 10):\r\n","                params['start'] = start\r\n","                raw = requests.get(Url, headers={'User-Agent': 'Mozilla/5.0'}, params=params)\r\n","\r\n","                dom = BeautifulSoup(raw.text, \"html.parser\")\r\n","\r\n","                lists = dom.select(\"#main_pack > section.sc_new.sp_nnews._prs_nws > div > div.group_news > ul > li\")\r\n","\r\n","                for l in lists:\r\n","                    urls.append(l.select(\"div.news_wrap > div.news_area > div.news_info > div.info_group > a\")[-1]['href'])\r\n","                \r\n","                print('page:', page, '/  code:', code, '/  num:', start, '\\r', end='')\r\n","                page += 1\r\n","                time.sleep(1)\r\n","\r\n","    # 기사 총 몇 개 클롤링 하는지 보여주기\r\n","    news_urls = [\r\n","         url\r\n","         for url in urls\r\n","         if 'https://news.naver.com/' in url\r\n","    ]\r\n","    return news_urls, params\r\n","\r\n","def save_article(news_urls, params):\r\n","    # 크롤링한 날짜, 제목, 본문 저장\r\n","    news = []\r\n","\r\n","    i = 1 # 몇 번째 기사를 크롤링하는지 체크\r\n","    \r\n","    tot_sum = 0 \r\n","    for url in tqdm(news_urls, desc = params['query']):\r\n","        raw = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\r\n","\r\n","        dom = BeautifulSoup(raw.text, \"html.parser\")\r\n","\r\n","        try:\r\n","            title = dom.select_one('#articleTitle').text.strip()\r\n","            date = dom.select_one('#main_content > div.article_header > div.article_info > div > span.t11').text.strip()[:10]\r\n","        except Exception:\r\n","            continue\r\n","        \r\n","        # <script> <style> 제거 (전처리)\r\n","        for s in dom(['script', 'style', 'img']):\r\n","            s.decompose()\r\n","\r\n","        # 뉴스본문 리턴\r\n","        news_content = dom.find('div', attrs = {'id': 'articleBodyContents'})\r\n","        # 뉴스본문에 대한 전처리\r\n","        # 각 line 별로 strip(), 태그 제거\r\n","        lines = [\r\n","            line.strip()\r\n","            for line in news_content.get_text().splitlines()\r\n","        ]\r\n","        news_content = ''.join(lines).replace('[경향신문]','')\r\n","\r\n","        dic = {\r\n","            'title' : title,\r\n","            'date' : date,\r\n","            'content' : news_content\r\n","        }\r\n","        news.append(dic)\r\n","        \r\n","        i += 1\r\n","        time.sleep(1)\r\n","        tot_sum += i\r\n","\r\n","    df = pd.DataFrame(news)\r\n","\r\n","    base_path = r'../../dataset'\r\n","    file_path = os.path.join(base_path, f\"{params['query']}_기사_크롤링_2020.01.01_{params['de']}.csv\")\r\n","\r\n","    df.to_csv(file_path, encoding='utf-8-sig')\r\n","    "],"outputs":[],"metadata":{"id":"eIi0gfA1CHZR"}},{"cell_type":"code","execution_count":8,"source":["from sys import stdout\r\n","import time\r\n","\r\n","# 병노\r\n","keyword_list = [\r\n","#  '일진머티리얼즈',\r\n","#  'LG화학',\r\n","#  '메리츠화재',\r\n","#  '삼성엔지니어링',\r\n","#  '한국가스공사',\r\n","#  '삼성중공업',\r\n","#  '한화솔루션',\r\n","#  '맥쿼리인프라',\r\n","#  '카카오',\r\n","#  'DB손해보험',\r\n","#  'NAVER',\r\n","#  '메리츠증권',\r\n","#  '기아',\r\n","#  '우리금융지주',\r\n","#  '현대차',\r\n","#  '이마트',\r\n","#  '대한항공',\r\n","#  '한솔케미칼',\r\n","#  '아모레G',\r\n","#  'LG디스플레이',\r\n","#  '한국타이어앤테크놀로지',\r\n","#  '현대건설',\r\n","#  '쌍용C&E',\r\n","#  '한온시스템',\r\n","#  '삼성전자',\r\n","#  '금호석유',\r\n","#  'S-Oil',\r\n","#  'POSCO',\r\n","#  'GS',\r\n","'네이버'\r\n"," ]\r\n"," \r\n","\r\n","news_urls = []\r\n","\r\n","cnt = 1\r\n","for keyword in keyword_list:\r\n","    print(cnt, \"/\", len(keyword_list), keyword)\r\n","    news_urls, params = crawling_article(keyword)\r\n","    save_article(news_urls, params)\r\n","    cnt += 1\r\n"],"outputs":[{"output_type":"stream","name":"stdout","text":["1 / 2 포스코\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-8-73557b9ede4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeyword_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeyword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mnews_urls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrawling_article\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0msave_article\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews_urls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mcnt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-1-af82e10f10ad>\u001b[0m in \u001b[0;36mcrawling_article\u001b[1;34m(key_word)\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'page:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/  code:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/  num:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[0mpage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;31m# 기사 총 몇 개 클롤링 하는지 보여주기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["# 전체 주식 정보 가져오기\r\n","import FinanceDataReader as fdr\r\n","\r\n","df = fdr.StockListing(\"KRX\")\r\n","df\r\n","\r\n","# 다음 시가총액 TOP 100 읽어오기\r\n","import requests\r\n","from bs4 import BeautifulSoup\r\n","\r\n","# 다음 시가총액 TOP 100\r\n","url = \"https://finance.daum.net/api/trend/market_capitalization?page={}&perPage=30&fieldName=marketCap&order=desc&market=KOSPI&pagination=true\"\r\n","\r\n","headers={\r\n","    \"User-Agent\" : \"Mozilla/5.0\",\r\n","    \"referer\" : \"https://finance.daum.net/domestic/market_cap?market=KOSPI\"\r\n","}\r\n","\r\n","stock_name = []\r\n","for i in range(1, 5):\r\n","    response = requests.get(url.format(i), headers=headers).json()  # json\r\n","    data = response['data']\r\n","\r\n","    if i < 4:\r\n","        for j in range(30):\r\n","            stock_name.append(data[j]['name'])\r\n","    else:\r\n","        for j in range(10):\r\n","            stock_name.append(data[j]['name'])\r\n","            \r\n","# stock_name.extend(['포스코', '케이티', '케이티앤지', '에스케이바이오팜', '금호석유화학','삼성화재해상보험', '쌍용씨앤이', '아모레퍼시픽그룹', '한국전력공사', '현대자동차'])\r\n","print(stock_name)\r\n","\r\n","stock_name_df = pd.DataFrame(stock_name)\r\n","\r\n","# 시가총액 TOP 100 중 상장일이 2020-01-01 이전인 주식 정보 가져오기\r\n","stock_df = df[df['Name'].isin(stock_name)]\r\n","stock_df = stock_df[stock_df['ListingDate']<='2020-01-01']\r\n","stock_df['Name']"],"outputs":[{"output_type":"stream","name":"stdout","text":["['삼성전자', 'SK하이닉스', 'NAVER', '삼성바이오로직스', '삼성전자우', '카카오', 'LG화학', '삼성SDI', '현대차', '기아', '카카오뱅크', '셀트리온', '카카오페이', 'POSCO', 'KB금융', '현대모비스', '크래프톤', '삼성물산', 'LG전자', '신한지주', 'SK이노베이션', 'SK', 'LG생활건강', 'SK바이오사이언스', '엔씨소프트', '한국전력', '삼성전기', '삼성생명', '하이브', 'HMM', '하나금융지주', 'LG', 'SK텔레콤', '삼성에스디에스', 'SK아이이테크놀로지', 'KT&G', '포스코케미칼', '두산중공업', '넷마블', '아모레퍼시픽', '대한항공', 'S-Oil', '현대중공업', '삼성화재', '우리금융지주', 'SK스퀘어', '고려아연', '기업은행', 'KT', 'LG디스플레이', '롯데케미칼', 'SK바이오팜', '한온시스템', 'SKC', '한국조선해양', '한화솔루션', 'LG이노텍', 'F&F', 'LG유플러스', '현대글로비스', '미래에셋증권', '현대제철', '코웨이', '현대건설', '맥쿼리인프라', '일진머티리얼즈', 'CJ제일제당', '금호석유', '에스디바이오센서', '강원랜드', 'KODEX 200', '한국타이어앤테크놀로지', '삼성중공업', '메리츠금융지주', '한국금융지주', '현대중공업지주', '삼성엔지니어링', '삼성증권', '유한양행', '두산밥캣', '이마트', '한진칼', '오리온', 'DB손해보험', '메리츠화재', '쌍용C&E', '삼성카드', 'NH투자증권', '아모레G', 'GS', '현대차2우B', '한미사이언스', '메리츠증권', '한솔케미칼', 'GS건설', '한국가스공사', '한전기술', 'GS리테일', '롯데지주', '한미약품']\n"]},{"output_type":"execute_result","data":{"text/plain":["63       CJ제일제당\n","77       DB손해보험\n","114          GS\n","116        GS건설\n","118       GS리테일\n","         ...   \n","7000     현대글로비스\n","7005      현대모비스\n","7027       현대제철\n","7029    현대중공업지주\n","7030        현대차\n","Name: Name, Length: 86, dtype: object"]},"metadata":{},"execution_count":8}],"metadata":{}},{"cell_type":"code","execution_count":23,"source":["import random\r\n","\r\n","stock_list = list(stock_df['Name'])\r\n","stock_list.pop(stock_list.index('삼성증권'))\r\n","random.shuffle(stock_list)  # 순서 바꿔주기\r\n","print(stock_list)\r\n","len(stock_list)"],"outputs":[{"output_type":"stream","name":"stdout","text":["['일진머티리얼즈', 'LG화학', '메리츠화재', '삼성엔지니어링', '한국가스공사', '삼성중공업', '한화솔루션', '맥쿼리인프라', '카카오', 'DB손해보험', 'NAVER', '메리츠증권', '기아', '우리금융지주', '현대차', '이마트', '대한항공', '한솔케미칼', '아모레G', 'LG디스플레이', '한국타이어앤테크놀로지', '현대건설', '쌍용C&E', '한온시스템', '삼성전자', '금호석유', 'S-Oil', 'POSCO', 'GS', 'LG전자', '한진칼', 'LG생활건강', '고려아연', '한미사이언스', 'LG', '현대제철', '롯데케미칼', '포스코케미칼', '현대중공업지주', 'KT', 'SK하이닉스', '신한지주', 'LG이노텍', 'KB금융', '넷마블', '삼성카드', 'SK텔레콤', '한미약품', '엔씨소프트', '한국조선해양', 'HMM', '코웨이', '한전기술', '롯데지주', '기업은행', 'SK이노베이션', '하나금융지주', '삼성SDI', '삼성화재', '두산중공업', 'CJ제일제당', '강원랜드', 'GS건설', '오리온', '한국전력', '현대글로비스', 'KT&G', 'SKC', '삼성바이오로직스', 'LG유플러스', '두산밥캣', '유한양행', '미래에셋증권', '한국금융지주', '삼성전기', '삼성물산', 'SK', '아모레퍼시픽', 'NH투자증권', '셀트리온', 'GS리테일', '삼성생명', '삼성에스디에스', '메리츠금융지주', '현대모비스']\n"]},{"output_type":"execute_result","data":{"text/plain":["85"]},"metadata":{},"execution_count":23}],"metadata":{}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}}],"metadata":{"colab":{"collapsed_sections":[],"name":"네이버 크롤링.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3.8.8 64-bit (conda)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"interpreter":{"hash":"b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"}},"nbformat":4,"nbformat_minor":5}