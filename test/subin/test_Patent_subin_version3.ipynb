{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a9b5971",
   "metadata": {},
   "source": [
    "---\n",
    "## [이용 가이드]\n",
    "\n",
    "#### 첫번째 셀 : 기본설정 및 크롤링\n",
    "- 사용자 설정 있음\n",
    "\n",
    "> `start_page` : 크롤링할 시작 페이지  \n",
    "> `resresh_unit` : 새로고침 단위 (320 페이지 이상 이동시 크롬 out of memory 발생  \n",
    "> `stop_page` : 크롤링할 마지막 페이지  \n",
    "> `file_name` : 저장할 파일명  \n",
    "- .기호 개당 1초를 의미 <- 사용자 정의 함수 delay(str, int)\n",
    "\n",
    "#### 두번째 셀 : 데이터 저장\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8e3f4f5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프로그램 시작\n",
      "인터넷 접속중\n",
      "크롤링 준비중...완료\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-5e30d93c8b35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;31m# 3.1. 첫 페이지 크롤링\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mcurrent_page\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontentCrawling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcurrent_page\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mstop_page\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m   \u001b[1;31m# 실행종료\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcurrent_page\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mstart_page\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m# 첫 페이지 찾기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\DevRoot\\StockHelper\\test\\subin\\c.py\u001b[0m in \u001b[0;36mcontentCrawling\u001b[1;34m(current_page_css_selector)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mcurrent_page_css_selector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'span.board_pager03 strong'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcontentCrawling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_page_css_selector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_page_css_selector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mdom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mpatent_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.search_section article'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "# 첫번째 셀 : 기본설정\n",
    "# 크롤링을 위해 사용자가 입력해야 되는 값\n",
    "start_page = 1    # 크롤링할 시작 페이지\n",
    "stop_page = 3   # 크롤링할 마지막 페이지\n",
    "refresh_unit = 5   # 크롤링 후 새로고침할 단위\n",
    "check_auto_save = True   # refresh_unit마다 파일 자동저장 활성화 여부\n",
    "file_name = 'patent_{date}_{first_page}_{last_page}.csv'  # 저장할 파일명\n",
    "\n",
    "# 메인\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "from math import ceil\n",
    "\n",
    "# 함수 : 딜레이\n",
    "def delay(text, sec):\n",
    "    print(text, end=\"\")\n",
    "    for i in range(sec): print('.', end=\"\"); time.sleep(1)\n",
    "\n",
    "# 함수 : 사이트에서 제목/내용 크롤링\n",
    "current_page_css_selector = 'span.board_pager03 strong'\n",
    "def contentCrawling(current_page_css_selector = current_page_css_selector):\n",
    "    dom = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    patent_list = dom.select('.search_section article')\n",
    "    \n",
    "    current_page = driver.find_element_by_css_selector(current_page_css_selector).text\n",
    "\n",
    "    result = [\n",
    "            {\n",
    "                '제목': patent.select_one('.search_section_title h1 > a:nth-of-type(2)').text.strip(),\n",
    "                '내용': patent.select_one('.search_txt').text.strip()\n",
    "            }\n",
    "            for patent in patent_list\n",
    "    ]\n",
    "\n",
    "    return int(current_page), result   # (크롤링한 페이지, 크롤링 결과) 반환\n",
    "\n",
    "# 함수 : 저장할 파일 경로 찾기\n",
    "def findPath(file_name):\n",
    "    file_path = r'D:\\DevRoot\\StockHelper\\dataset'\n",
    "    file_name = file_name\n",
    "    try: \n",
    "        path = os.path.join(file_path, file_name)\n",
    "    except(Exception):\n",
    "        file_path = 'C:\\DeepLearning_Project\\StockHelper\\StockHelper\\dataset'\n",
    "        path = os.path.join(file_path, file_name)\n",
    "        \n",
    "    return path\n",
    "\n",
    "# 함수 : 파일저장\n",
    "# 데이터 저장 : list -> df -> csv 저장\n",
    "def saveFile(path, data, start_index=0, last_index=len(data)):\n",
    "    pd.DataFrame(data, index=range(start_index, last_index)).to_csv(path, encoding='utf-8')\n",
    "    # 저장 결과 반환\n",
    "    if os.path.isfile(path):\n",
    "        print('>>> 파일변환 완료:', datetime.today().strftime((\"%Y-%m-%d %H:%M:%S\")))\n",
    "        print('>>> 저장위치:', path)\n",
    "    else: print('>>> 파일변환 실패')\n",
    "\n",
    "print('프로그램 시작')\n",
    "start = time.time()  # 시작 시간 저장\n",
    "result = []\n",
    "check_page_list = []   # 실제 페이지 이동 결과 (페이지 이동의 중복/누락 확인용)\n",
    "check_save_point = False   # 파일 저장하는 시점 확인\n",
    "\n",
    "# 1. webdriver를 이용해 kipris 접속\n",
    "driver_path = r'D:\\DevRoot\\download\\chromedriver.exe'\n",
    "try: \n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "except(Exception):\n",
    "    driver_path = r'C:\\DevRoot\\download\\chromedriver.exe'\n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "finally:\n",
    "    driver.implicitly_wait(10)   # 웹 페이지 로딩 완료 최대 대기 시간 (한번만 설정하면 driver를 사용하는 모든 코드에 적용)\n",
    "\n",
    "for refreshed_num in range(1, ceil(stop_page / refresh_unit) + 1):\n",
    "    print('인터넷 접속중')\n",
    "    driver.get(\"http://kpat.kipris.or.kr/kpat/searchLogina.do?next=MainSearch\")\n",
    "    \n",
    "    # 크롤링 시작페이지 재설정\n",
    "    if refreshed_num != 1: start_page = refresh_unit * (refreshed_num - 1) + 1\n",
    "    \n",
    "    # 2. 검색 옵션 설정\n",
    "    # 2.1. 행정상태 변경\n",
    "    # defalut 해제 <- '전체' 클릭\n",
    "    driver.find_element_by_css_selector('form#leftside .release_list > span:nth-of-type(1) > input').click()\n",
    "    # 원하는 checkbox만 선택 <- '등록' 클릭\n",
    "    driver.find_element_by_css_selector('form#leftside .release_list > span:nth-last-of-type(1) > input').click()\n",
    "\n",
    "    # 2.2. 기간을 검색어로 입력\n",
    "    today = datetime.today().strftime(\"%Y%m%d\")\n",
    "    decade = str(int(today) - int('00001000'))\n",
    "    driver.find_element_by_css_selector('.keyword').send_keys(f'GD=[{decade}~{today}]')\n",
    "    driver.find_element_by_css_selector('.input_btn img').click()\n",
    "\n",
    "    # 2.3. 90개씩 보기 선택\n",
    "    pageSel = 90   # 페이지당 게시물 개수 (30, 60, 90 중 택1)\n",
    "    select = Select(driver.find_element_by_id('opt28'))\n",
    "    select.select_by_value(str(pageSel))\n",
    "    driver.find_element_by_css_selector('#pageSel img').click()\n",
    "\n",
    "    # 3. 데이터 추출\n",
    "    delay('크롤링 준비중', 3); print('완료')\n",
    "    page_num = 'span.board_pager03 a:nth-last-of-type({0})'   # target_page 구할 때 이용\n",
    "\n",
    "    # 3.1. 첫 페이지 크롤링\n",
    "    current_page, data = contentCrawling()\n",
    "    if current_page == stop_page: break   # 실행종료\n",
    "    if current_page != start_page:   # 첫 페이지 찾기\n",
    "        delay('시작 페이지로 이동중', 4)\n",
    "        while current_page < start_page:\n",
    "            if current_page // 10 < start_page // 10:   # 10 페이지씩 다음 페이지로 이동\n",
    "                driver.find_element_by_css_selector(page_num.format(1)).click()\n",
    "                delay('', 2)\n",
    "                current_page = int(driver.find_element_by_css_selector(current_page_css_selector).text)\n",
    "                continue\n",
    "            for i in range(10, 0, -1):   # 한 페이지씩 다음 페이지로 이동\n",
    "                driver.find_element_by_css_selector(page_num.format(i)).click()\n",
    "                delay('', 2)\n",
    "                current_page = int(driver.find_element_by_css_selector(current_page_css_selector).text)\n",
    "                if current_page == start_page: print('완료'); break\n",
    "    current_page, data = contentCrawling()\n",
    "    result.extend(data)\n",
    "    print(f'{current_page} 위치 -> {current_page} 페이지 크롤링 완료')\n",
    "    check_page_list.append(current_page)\n",
    "\n",
    "    # 3.2. 페이지 이동하며 크롤링\n",
    "    while True:\n",
    "        # 실행 종료\n",
    "        if current_page >= stop_page: break\n",
    "        if current_page == refresh_unit * refreshed_num: break\n",
    "\n",
    "        for i in range(10, 0, -1):\n",
    "            if current_page >= stop_page: break\n",
    "            if current_page == refresh_unit * refreshed_num: check_save_point = True; break\n",
    "\n",
    "            # 3.2.1. 크롤링할 페이지(target_page)가 현재 페이지의 다음 페이지인 지 확인\n",
    "            target_page = driver.find_element_by_css_selector(page_num.format(i))\n",
    "            if i != 1 and int(target_page.text) <= current_page: delay('', 1); continue \n",
    "#             if i != 1 and int(target_page.text) > current_page + 1: i -= 1; continue\n",
    "\n",
    "            print(f'현재 {current_page}', end=\" -> \")\n",
    "            # 3.2.2. 크롤링할 페이지(target_page)인 다음 페이지로 이동\n",
    "            try:\n",
    "                print(f'{target_page.text} click', end=\" -> \")\n",
    "            except Exception:\n",
    "                driver.refresh()   # 새로고침\n",
    "                dealy('', 5)\n",
    "                dom = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                patent_list = dom.select('.search_section article')\n",
    "            finally:\n",
    "                target_page.click()  # 크롤링할 페이지로 이동\n",
    "                delay('', 2)\n",
    "            while True:\n",
    "                try: # (클릭해서 이동한) 현재 페이지(click_page)는 target_page\n",
    "                    click_page = driver.find_element_by_css_selector(current_page_css_selector).text\n",
    "    #             except KeyboardInterrupt or WebDriverException:\n",
    "    #                 print('Interrupted')\n",
    "    #                 try: sys.exit(0)\n",
    "    #                 except SystemExit: os._exit(0)\n",
    "    #                 finally:\n",
    "    #                     print('긁어온 데이터개수 :', len(result))\n",
    "    #                     print('소요시간 :', int(time.time() - start) / 60, '분')  # 현재시각 - 시작시간 = 실행 시간\n",
    "    #                     print('프로그램 종료')\n",
    "                except Exception:\n",
    "                    driver.refresh()   # 새로고침\n",
    "                    dom = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    patent_list = dom.select('.search_section article')\n",
    "                    click_page = int(driver.find_element_by_css_selector(current_page_css_selector).text)\n",
    "\n",
    "                if current_page != click_page: break  # 페이지 이동 시 loop 탈출\n",
    "            # 3.2.3. 크롤링\n",
    "            try:\n",
    "                current_page, data = contentCrawling()\n",
    "            except Exception:\n",
    "                driver.refresh()   # 새로고침\n",
    "                current_page, data = contentCrawling()\n",
    "            finally:\n",
    "                result.extend(data)\n",
    "            # 3.2.4. 중간결과 반환\n",
    "            print(f'{current_page} 페이지 크롤링 완료')\n",
    "            check_page_list.append(current_page)\n",
    "            \n",
    "    # 4.1. 파일 저장\n",
    "    if check_save_point and check_auto_save:\n",
    "        print('=' * 60)\n",
    "        file_name = file_name.format(date=today, first_page=start_page, last_page=stop_page)\n",
    "        path = findPath(file_name)\n",
    "        saveFile(result, (start_page - 1) * pageSel, stop_page * pageSel, path)\n",
    "    \n",
    "        # 4.2. 저장 결과\n",
    "        print('예상 데이터 개수 :', (stop_page - start_page + 1) * pageSel)\n",
    "        print('긁어온 데이터개수 :', len(result))\n",
    "        normal_page_list = [i for i in range(start_page, current_page + 1)]   # 정상적으로 크롤링했을 때\n",
    "        num = len(normal_page_list)\n",
    "        for i in range(start_page, current_page + 1):\n",
    "            if i in check_page_list:\n",
    "                check_page_list.pop(check_page_list.index(i))\n",
    "                normal_page_list.pop(normal_page_list.index(i))\n",
    "        print('중복된 페이지 :', check_page_list if check_page_list else '없음')\n",
    "        print('누락된 페이지 :', normal_page_list if normal_page_list else '없음')\n",
    "        print('=' * 60)\n",
    "        check_page_list = []\n",
    "        normal_page_list = []\n",
    "        check_save_point = False\n",
    "        result = []\n",
    "        \n",
    "\n",
    "print('=' * 60)\n",
    "print('소요시간 :', int(time.time() - start) / 60, '분')  # 현재시각 - 시작시간 = 실행 시간\n",
    "file_name = file_name.format(date=today, first_page=start_page, last_page=stop_page)\n",
    "path = findPath(file_name)\n",
    "saveFile(result, (start_page - 1) * pageSel, stop_page * pageSel, path)\n",
    "print('예상 데이터 개수 :', (stop_page - start_page + 1) * pageSel)\n",
    "print('긁어온 데이터개수 :', len(result))\n",
    "normal_page_list = [i for i in range(start_page, current_page + 1)]   # 정상적으로 크롤링했을 때\n",
    "num = len(normal_page_list)\n",
    "for i in range(start_page, current_page + 1):\n",
    "    if i in check_page_list:\n",
    "        check_page_list.pop(check_page_list.index(i))\n",
    "        normal_page_list.pop(normal_page_list.index(i))\n",
    "print('중복된 페이지 :', check_page_list if check_page_list else '없음')\n",
    "print('누락된 페이지 :', normal_page_list if normal_page_list else '없음')\n",
    "print('프로그램 종료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1565cbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 파일변환 완료: 2021-12-03 09:12:15\n",
      ">>> 저장위치: D:\\DevRoot\\StockHelper\\dataset\\patent_20211203_1_1111.0.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 두번째 셀 : 데이터 저장\n",
    "# 데이터 저장 : list -> df -> csv 저장\n",
    "file_name = file_name.format(date=today, first_page=start_page, last_page=current_page)\n",
    "path = findPath(file_name)\n",
    "pd.DataFrame(result, index=range(1, len(result) + 1)).to_csv(path, encoding='utf-8')\n",
    "\n",
    "# 저장 결과 반환\n",
    "if os.path.isfile(path):\n",
    "    print('>>> 파일변환 완료:', datetime.today().strftime((\"%Y-%m-%d %H:%M:%S\")))\n",
    "    print('>>> 저장위치:', path)\n",
    "else: print('>>> 파일변환 실패')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
