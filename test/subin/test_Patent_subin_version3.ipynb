{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a9b5971",
   "metadata": {},
   "source": [
    "---\n",
    "## [이용 가이드]\n",
    "\n",
    "#### 초기 기본설정\n",
    "- `start_page` : 크롤링할 시작 페이지  \n",
    "- `resresh_unit` : 크롤링 후 새로고침 단위\n",
    "> - 참고 : <mark>320 페이지 이상 이동시 크롬 out of memory 발생</mark>  \n",
    "- `stop_page` : 크롤링할 마지막 페이지  \n",
    "- `check_auto_save` : 중간 저장 활성화 여부\n",
    "> - True : start_page ~ stop_page까지 refresh_unit 마다 크롤링 결과를 n개의 파일로 저장  \n",
    "> - False : start_page ~ stop_page까지 크롤링한 결과를 1개의 파일로 저장\n",
    "- `file_name` : 저장할 파일명\n",
    "- `firstPage` : check_auto_save 설정과 관련되어, 시작 페이지에 대한 변수\n",
    "> - 참고 : <mark>값 변경하지 않기</mark>\n",
    "\n",
    "#### 사용자 정의 함수\n",
    "- delay(str, int)\n",
    "> 1. time.sleep() 의 진행을 보고싶을 떄 사용\n",
    "> 1. 1초씩 총 int 초만큼 딜레이 시킴, 1초마다 . 출력\n",
    "> 1. 'str:.....'형식으로 출력\n",
    "> 1. return 없음\n",
    "- contentCrawling()\n",
    "> 1. 현재 페이지의 제목과 내용을 크롤링하고 싶을 때 사용\n",
    "> 1. 디폴트 매개변수 가짐, 디폴트에 들어갈 값은 함수 위에 선언 -> 매개변수 없이 호출 가능\n",
    "> 1. return int, list (크롤링한 페이지, 크롤링 결과) 반환\n",
    "- findPath(fileName)\n",
    "> 1. 파일 저장 경로를 찾고싶을 떄 사용\n",
    "> 1. return path (입력되어져 있는 저장경로에 파일 이름을 붙여서 반환)\n",
    "> 1. fileName에 확장자도 함께 넣어줘야 함\n",
    "- saveFile(path, data, start_index, last_index)\n",
    "> 1. 파일을 저장하고 싶을 때 사용\n",
    "> 1. 저장하려는 data를 path 경로에 저장 (해당 데이터의 인덱스 설정 필수)\n",
    "> 1. 매개변수 타입 : str, iterable, int, int\n",
    "> 1. return 없음 (성공여부 출력)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec11867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 셀 : 기본설정\n",
    "# 크롤링을 위해 사용자가 입력해야 되는 값\n",
    "start_page = 1\n",
    "refresh_unit = 200\n",
    "stop_page = 1111\n",
    "check_auto_save = True\n",
    "file_name = 'patent_{date}_{first_page}_{last_page}.csv'\n",
    "firstPage = start_page   # 값 변경하면 안 됨\n",
    "\n",
    "# 메인\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "from math import ceil\n",
    "\n",
    "# 함수 : 딜레이\n",
    "def delay(text, sec):\n",
    "    print(text, end=\"\")\n",
    "    for i in range(sec): print('.', end=\"\"); time.sleep(1)\n",
    "\n",
    "# 함수 : 사이트에서 제목/내용 크롤링\n",
    "current_page_css_selector = 'span.board_pager03 strong'\n",
    "def contentCrawling(current_page_css_selector = current_page_css_selector):\n",
    "    dom = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    patent_list = dom.select('.search_section article')\n",
    "    \n",
    "    current_page = driver.find_element_by_css_selector(current_page_css_selector).text\n",
    "\n",
    "    result = [\n",
    "            {\n",
    "                '제목': patent.select_one('.search_section_title h1 > a:nth-of-type(2)').text.strip(),\n",
    "                '내용': patent.select_one('.search_txt').text.strip()\n",
    "            }\n",
    "            for patent in patent_list\n",
    "    ]\n",
    "\n",
    "    return int(current_page), result   # (크롤링한 페이지, 크롤링 결과) 반환\n",
    "\n",
    "# 함수 : 저장할 파일 경로 찾기\n",
    "def findPath(fileName):\n",
    "    file_path = r'D:\\DevRoot\\StockHelper\\dataset'\n",
    "    try: \n",
    "        path = os.path.join(file_path, fileName)\n",
    "    except(Exception):\n",
    "        file_path = 'C:\\DeepLearning_Project\\StockHelper\\StockHelper\\dataset'\n",
    "        path = os.path.join(file_path, fileName)\n",
    "        \n",
    "    return path\n",
    "\n",
    "# 함수 : 파일저장\n",
    "# 데이터 저장 : list -> df -> csv 저장\n",
    "def saveFile(path, data, start_index, last_index):\n",
    "    pd.DataFrame(data, index=range(start_index, last_index)).to_csv(path, encoding='utf-8')\n",
    "    # 저장 결과 반환\n",
    "    if os.path.isfile(path):\n",
    "        print('>>> 파일변환 완료:', datetime.today().strftime((\"%Y-%m-%d %H:%M:%S\")))\n",
    "        print('>>> 저장위치:', path)\n",
    "    else: print('>>> 파일변환 실패')\n",
    "\n",
    "print('프로그램 시작')\n",
    "start = time.time()  # 시작 시간 저장\n",
    "result = []\n",
    "check_page_list = []   # 실제 페이지 이동 결과 (페이지 이동의 중복/누락 확인용)\n",
    "check_save_point = False   # 파일 저장하는 시점 확인\n",
    "\n",
    "# 1. webdriver를 이용해 kipris 접속\n",
    "driver_path = r'D:\\DevRoot\\download\\chromedriver.exe'\n",
    "try: \n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "except(Exception):\n",
    "    driver_path = r'C:\\DevRoot\\download\\chromedriver.exe'\n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "finally:\n",
    "    driver.implicitly_wait(10)   # 웹 페이지 로딩 완료 최대 대기 시간 (한번만 설정하면 driver를 사용하는 모든 코드에 적용)\n",
    "\n",
    "if refresh_unit <= 0: refresh_unit = 100000000\n",
    "for refreshed_num in range(1, ceil(stop_page / refresh_unit) + 1):\n",
    "    print('인터넷 접속중')\n",
    "    driver.get(\"http://kpat.kipris.or.kr/kpat/searchLogina.do?next=MainSearch\")\n",
    "    \n",
    "    # 크롤링 시작페이지 재설정\n",
    "    if refreshed_num != 1: start_page = refresh_unit * (refreshed_num - 1) + 1\n",
    "    \n",
    "    # 2. 검색 옵션 설정\n",
    "    # 2.1. 행정상태 변경\n",
    "    # defalut 해제 <- '전체' 클릭\n",
    "    driver.find_element_by_css_selector('form#leftside .release_list > span:nth-of-type(1) > input').click()\n",
    "    # 원하는 checkbox만 선택 <- '등록' 클릭\n",
    "    driver.find_element_by_css_selector('form#leftside .release_list > span:nth-last-of-type(1) > input').click()\n",
    "\n",
    "    # 2.2. 기간을 검색어로 입력\n",
    "    today = datetime.today().strftime(\"%Y%m%d\")\n",
    "    decade = str(int(today) - int('00001000'))\n",
    "    driver.find_element_by_css_selector('.keyword').send_keys(f'GD=[{decade}~{today}]')\n",
    "    driver.find_element_by_css_selector('.input_btn img').click()\n",
    "\n",
    "    # 2.3. 90개씩 보기 선택\n",
    "    pageSel = 90   # 페이지당 게시물 개수 (30, 60, 90 중 택1)\n",
    "    select = Select(driver.find_element_by_id('opt28'))\n",
    "    select.select_by_value(str(pageSel))\n",
    "    driver.find_element_by_css_selector('#pageSel img').click()\n",
    "\n",
    "    # 3. 데이터 추출\n",
    "    delay('크롤링 준비중', 3); print('완료')\n",
    "    page_num = 'span.board_pager03 a:nth-last-of-type({0})'   # target_page 구할 때 이용\n",
    "\n",
    "    # 3.1. 첫 페이지 크롤링\n",
    "    current_page, data = contentCrawling()\n",
    "    if current_page == stop_page: break   # 실행종료\n",
    "    if current_page != start_page:   # 첫 페이지 찾기\n",
    "        delay('시작 페이지로 이동', 0)\n",
    "        while current_page < start_page:\n",
    "            if current_page // 10 < start_page // 10:\n",
    "                driver.find_element_by_css_selector(page_num.format(1)).click()\n",
    "                delay('', 2)\n",
    "                current_page = int(driver.find_element_by_css_selector(current_page_css_selector).text)\n",
    "                continue\n",
    "            for i in range(10, 0, -1):\n",
    "                driver.find_element_by_css_selector(page_num.format(i)).click()\n",
    "                delay('', 2)\n",
    "                current_page = int(driver.find_element_by_css_selector(current_page_css_selector).text)\n",
    "                if current_page == start_page: print('완료'); break\n",
    "    current_page, data = contentCrawling()\n",
    "    delay('', 2)\n",
    "    result.extend(data)\n",
    "    print(f'{current_page} 위치 -> {current_page} 페이지 크롤링 완료 / 누적 {len(result)} 건')\n",
    "    check_page_list.append(current_page)\n",
    "\n",
    "    # 3.2. 페이지 이동하며 크롤링\n",
    "    while True:\n",
    "        # 실행 종료\n",
    "        if current_page >= stop_page: break\n",
    "        if current_page == refresh_unit * refreshed_num: break\n",
    "\n",
    "        for i in range(10, 0, -1):\n",
    "            if current_page >= stop_page: break\n",
    "            if current_page == refresh_unit * refreshed_num: check_save_point = True; break\n",
    "\n",
    "            # 3.2.1. 크롤링할 페이지(target_page)가 현재 페이지의 다음 페이지인 지 확인\n",
    "            target_page = driver.find_element_by_css_selector(page_num.format(i))\n",
    "            if i != 1 and int(target_page.text) <= current_page: delay('', 1); continue \n",
    "\n",
    "            print(f'현재 {current_page}', end=\" -> \")\n",
    "            dataLen1 = current_page * pageSel   # 현재 페이지의 크롤링 데이터 개수\n",
    "            # 3.2.2. 크롤링할 페이지(target_page)인 다음 페이지로 이동\n",
    "            try:\n",
    "                print(f'{target_page.text} click', end=\" -> \")\n",
    "            except Exception:\n",
    "                driver.refresh()   # 새로고침\n",
    "                dealy('', 5)\n",
    "                dom = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                patent_list = dom.select('.search_section article')\n",
    "            finally:\n",
    "                target_page.click()  # 크롤링할 페이지로 이동\n",
    "                delay('', 2)\n",
    "            while True:\n",
    "                try: # (클릭해서 이동한) 현재 페이지(click_page)는 target_page\n",
    "                    click_page = driver.find_element_by_css_selector(current_page_css_selector).text\n",
    "    #             except KeyboardInterrupt or WebDriverException:\n",
    "    #                 print('Interrupted')\n",
    "    #                 try: sys.exit(0)\n",
    "    #                 except SystemExit: os._exit(0)\n",
    "    #                 finally:\n",
    "    #                     print('긁어온 데이터개수 :', len(result))\n",
    "    #                     print('소요시간 :', int(time.time() - start) / 60, '분')  # 현재시각 - 시작시간 = 실행 시간\n",
    "    #                     print('프로그램 종료')\n",
    "                except Exception:\n",
    "                    driver.refresh()   # 새로고침\n",
    "                    dom = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    patent_list = dom.select('.search_section article')\n",
    "                    click_page = int(driver.find_element_by_css_selector(current_page_css_selector).text)\n",
    "\n",
    "                if current_page != click_page: break  # 페이지 이동 시 loop 탈출\n",
    "            # 3.2.3. 크롤링\n",
    "            try:\n",
    "                current_page, data = contentCrawling()\n",
    "            except Exception:\n",
    "                driver.refresh()   # 새로고침\n",
    "                current_page, data = contentCrawling()\n",
    "            finally:\n",
    "                result.extend(data)\n",
    "            # 3.2.4. 중간결과 반환\n",
    "            dataLen2 = current_page * pageSel   # 현재 페이지의 크롤링 데이터 개수\n",
    "            print(f'{current_page} ({dataLen2 - dataLen1}건) 크롤링 완료 / 누적 {len(result)} 건')\n",
    "            check_page_list.append(current_page)\n",
    "            \n",
    "    # 4.1. 파일 저장\n",
    "    if check_save_point and check_auto_save:\n",
    "        print('=' * 60)\n",
    "        fileName = file_name.format(date=today, first_page=start_page, last_page=current_page)\n",
    "        saveFile(path=findPath(fileName), data=result, start_index=(start_page - 1) * pageSel + 1, last_index=current_page * pageSel + 1)\n",
    "        \n",
    "        # 4.2. 저장 결과\n",
    "        print('예상 데이터 개수 :', (current_page - start_page + 1) * pageSel)\n",
    "        print('긁어온 데이터개수 :', len(result))\n",
    "        normal_page_list = [i for i in range(start_page, current_page + 1)]   # 정상적으로 크롤링했을 때\n",
    "        num = len(normal_page_list)\n",
    "        for i in range(start_page, current_page + 1):\n",
    "            if i in check_page_list:\n",
    "                check_page_list.pop(check_page_list.index(i))\n",
    "                normal_page_list.pop(normal_page_list.index(i))\n",
    "        print('중복된 페이지 :', check_page_list if check_page_list else '없음')\n",
    "        print('누락된 페이지 :', normal_page_list if normal_page_list else '없음')\n",
    "        print('=' * 60)\n",
    "        check_page_list = []\n",
    "        normal_page_list = []\n",
    "        check_save_point = False\n",
    "        result = []\n",
    "        \n",
    "\n",
    "print('=' * 60)\n",
    "if not check_auto_save: start_page = firstPage\n",
    "fileName = file_name.format(date=today, first_page=start_page, last_page=stop_page)\n",
    "saveFile(path=findPath(fileName), data=result, start_index=(start_page - 1) * pageSel + 1, last_index=stop_page * pageSel + 1)\n",
    "print('예상 데이터 개수 :', (stop_page - start_page + 1) * pageSel)\n",
    "print('긁어온 데이터개수 :', len(result))\n",
    "normal_page_list = [i for i in range(start_page, current_page + 1)]   # 정상적으로 크롤링했을 때\n",
    "num = len(normal_page_list)\n",
    "for i in range(start_page, current_page + 1):\n",
    "    if i in check_page_list:\n",
    "        check_page_list.pop(check_page_list.index(i))\n",
    "        normal_page_list.pop(normal_page_list.index(i))\n",
    "print('중복된 페이지 :', check_page_list if check_page_list else '없음')\n",
    "print('누락된 페이지 :', normal_page_list if normal_page_list else '없음')\n",
    "print('소요시간 :', int(time.time() - start) / 60, '분')  # 현재시각 - 시작시간 = 실행 시간\n",
    "print('프로그램 종료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443bf821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
