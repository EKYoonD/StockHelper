{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a9b5971",
   "metadata": {},
   "source": [
    "---\n",
    "## [이용 가이드]\n",
    "\n",
    "#### 초기 기본설정\n",
    "- `start_page` : 크롤링할 시작 페이지  \n",
    "- `resresh_unit` : 크롤링 후 새로고침 단위\n",
    "> - 참고 : <mark>320 페이지 이상 이동시 크롬 out of memory 발생</mark>  \n",
    "- `stop_page` : 크롤링할 마지막 페이지  \n",
    "- `file_name` : 저장할 파일명\n",
    "\n",
    "#### 사용자 정의 함수\n",
    "- delay(str, int)\n",
    "> 1. time.sleep() 의 진행을 보고싶을 떄 사용\n",
    "> 1. 1초씩 총 int 초만큼 딜레이 시킴, 1초마다 . 출력\n",
    "> 1. 'str:.....'형식으로 출력\n",
    "> 1. return 없음\n",
    "- contentCrawling()\n",
    "> 1. 현재 페이지의 제목과 내용을 크롤링하고 싶을 때 사용\n",
    "> 1. 디폴트 매개변수 가짐, 디폴트에 들어갈 값은 함수 위에 선언 -> 매개변수 없이 호출 가능\n",
    "> 1. return int, list (크롤링한 페이지, 크롤링 결과) 반환\n",
    "- findPath(fileName)\n",
    "> 1. 파일 저장 경로를 찾고싶을 떄 사용\n",
    "> 1. return path (입력되어져 있는 저장경로에 파일 이름을 붙여서 반환)\n",
    "> 1. fileName에 확장자도 함께 넣어줘야 함\n",
    "- saveFile(path, data)\n",
    "> 1. data를 path 경로에 저장할 떄 이용\n",
    "> 1. 매개변수 타입 : str, iterable\n",
    "> 1. return 없음 (성공여부 출력)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8ec11867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프로그램 시작\n",
      "인터넷 접속중\n",
      "크롤링 준비중...완료\n",
      "시작 페이지로 이동..........................................................................................................................................................................................................1001 위치 -> 1001 페이지 크롤링 완료 / 누적 90 건\n",
      "현재 1001 -> 1002 click -> ..1002 크롤링 완료 / 누적 180 건\n",
      "현재 1002 -> 1003 click -> ..1003 크롤링 완료 / 누적 270 건\n",
      "현재 1003 -> 1004 click -> ..1004 크롤링 완료 / 누적 360 건\n",
      "현재 1004 -> 1005 click -> ..1005 크롤링 완료 / 누적 450 건\n",
      "현재 1005 -> 1006 click -> ..1006 크롤링 완료 / 누적 540 건\n",
      "현재 1006 -> 1007 click -> ..1007 크롤링 완료 / 누적 630 건\n",
      "현재 1007 -> 1008 click -> ..1008 크롤링 완료 / 누적 720 건\n",
      "현재 1008 -> 1009 click -> ..1009 크롤링 완료 / 누적 810 건\n",
      "현재 1009 -> 1010 click -> ..1010 크롤링 완료 / 누적 900 건\n",
      "현재 1010 -> 다음 click -> ..1011 크롤링 완료 / 누적 990 건\n",
      "현재 1011 -> 1012 click -> ..1012 크롤링 완료 / 누적 1080 건\n",
      "현재 1012 -> 1013 click -> ..1013 크롤링 완료 / 누적 1170 건\n",
      "현재 1013 -> 1014 click -> ..1014 크롤링 완료 / 누적 1260 건\n",
      "현재 1014 -> 1015 click -> ..1015 크롤링 완료 / 누적 1350 건\n",
      "현재 1015 -> 1016 click -> ..1016 크롤링 완료 / 누적 1440 건\n",
      "현재 1016 -> 1017 click -> ..1017 크롤링 완료 / 누적 1530 건\n",
      "현재 1017 -> 1018 click -> ..1018 크롤링 완료 / 누적 1620 건\n",
      "현재 1018 -> 1019 click -> ..1019 크롤링 완료 / 누적 1710 건\n",
      "현재 1019 -> 1020 click -> ..1020 크롤링 완료 / 누적 1800 건\n",
      "현재 1020 -> 다음 click -> ..1021 크롤링 완료 / 누적 1890 건\n",
      "현재 1021 -> 1022 click -> ..1022 크롤링 완료 / 누적 1980 건\n",
      "현재 1022 -> 1023 click -> ..1023 크롤링 완료 / 누적 2070 건\n",
      "현재 1023 -> 1024 click -> ..1024 크롤링 완료 / 누적 2160 건\n",
      "현재 1024 -> 1025 click -> ..1025 크롤링 완료 / 누적 2250 건\n",
      "현재 1025 -> 1026 click -> ..1026 크롤링 완료 / 누적 2340 건\n",
      "현재 1026 -> 1027 click -> ..1027 크롤링 완료 / 누적 2430 건\n",
      "현재 1027 -> 1028 click -> ..1028 크롤링 완료 / 누적 2520 건\n",
      "현재 1028 -> 1029 click -> ..1029 크롤링 완료 / 누적 2610 건\n",
      "현재 1029 -> 1030 click -> ..1030 크롤링 완료 / 누적 2700 건\n",
      "현재 1030 -> 다음 click -> ..1031 크롤링 완료 / 누적 2790 건\n",
      "현재 1031 -> 1032 click -> ..1032 크롤링 완료 / 누적 2880 건\n",
      "현재 1032 -> 1033 click -> ..1033 크롤링 완료 / 누적 2970 건\n",
      "현재 1033 -> 1034 click -> ..1034 크롤링 완료 / 누적 3060 건\n",
      "현재 1034 -> 1035 click -> ..1035 크롤링 완료 / 누적 3150 건\n",
      "현재 1035 -> 1036 click -> ..1036 크롤링 완료 / 누적 3240 건\n",
      "현재 1036 -> 1037 click -> ..1037 크롤링 완료 / 누적 3330 건\n",
      "현재 1037 -> 1038 click -> ..1038 크롤링 완료 / 누적 3420 건\n",
      "현재 1038 -> 1039 click -> ..1039 크롤링 완료 / 누적 3510 건\n",
      "현재 1039 -> 1040 click -> ..1040 크롤링 완료 / 누적 3600 건\n",
      "현재 1040 -> 다음 click -> ..1041 크롤링 완료 / 누적 3690 건\n",
      "현재 1041 -> 1042 click -> ..1042 크롤링 완료 / 누적 3780 건\n",
      "현재 1042 -> 1043 click -> ..1043 크롤링 완료 / 누적 3870 건\n",
      "현재 1043 -> 1044 click -> ..1044 크롤링 완료 / 누적 3960 건\n",
      "현재 1044 -> 1045 click -> ..1045 크롤링 완료 / 누적 4050 건\n",
      "현재 1045 -> 1046 click -> ..1046 크롤링 완료 / 누적 4140 건\n",
      "현재 1046 -> 1047 click -> ..1047 크롤링 완료 / 누적 4230 건\n",
      "현재 1047 -> 1048 click -> ..1048 크롤링 완료 / 누적 4320 건\n",
      "현재 1048 -> 1049 click -> ..1049 크롤링 완료 / 누적 4410 건\n",
      "현재 1049 -> 1050 click -> ..1050 크롤링 완료 / 누적 4500 건\n",
      "현재 1050 -> 다음 click -> ..1051 크롤링 완료 / 누적 4590 건\n",
      "현재 1051 -> 1052 click -> ..1052 크롤링 완료 / 누적 4680 건\n",
      "현재 1052 -> 1053 click -> ..1053 크롤링 완료 / 누적 4770 건\n",
      "현재 1053 -> 1054 click -> ..1054 크롤링 완료 / 누적 4860 건\n",
      "현재 1054 -> 1055 click -> ..1055 크롤링 완료 / 누적 4950 건\n",
      "현재 1055 -> 1056 click -> ..1056 크롤링 완료 / 누적 5040 건\n",
      "현재 1056 -> 1057 click -> ..1057 크롤링 완료 / 누적 5130 건\n",
      "현재 1057 -> 1058 click -> ..1058 크롤링 완료 / 누적 5220 건\n",
      "현재 1058 -> 1059 click -> ..1059 크롤링 완료 / 누적 5310 건\n",
      "현재 1059 -> 1060 click -> ..1060 크롤링 완료 / 누적 5400 건\n",
      "현재 1060 -> 다음 click -> ..1061 크롤링 완료 / 누적 5490 건\n",
      "현재 1061 -> 1062 click -> ..1062 크롤링 완료 / 누적 5580 건\n",
      "현재 1062 -> 1063 click -> ..1063 크롤링 완료 / 누적 5670 건\n",
      "현재 1063 -> 1064 click -> ..1064 크롤링 완료 / 누적 5760 건\n",
      "현재 1064 -> 1065 click -> ..1065 크롤링 완료 / 누적 5850 건\n",
      "현재 1065 -> 1066 click -> ..1066 크롤링 완료 / 누적 5940 건\n",
      "현재 1066 -> 1067 click -> ..1067 크롤링 완료 / 누적 6030 건\n",
      "현재 1067 -> 1068 click -> ..1068 크롤링 완료 / 누적 6120 건\n",
      "현재 1068 -> 1069 click -> ..1069 크롤링 완료 / 누적 6210 건\n",
      "현재 1069 -> 1070 click -> ..1070 크롤링 완료 / 누적 6300 건\n",
      "현재 1070 -> 다음 click -> ..1071 크롤링 완료 / 누적 6390 건\n",
      "현재 1071 -> 1072 click -> ..1072 크롤링 완료 / 누적 6480 건\n",
      "현재 1072 -> 1073 click -> ..1073 크롤링 완료 / 누적 6570 건\n",
      "현재 1073 -> 1074 click -> ..1074 크롤링 완료 / 누적 6660 건\n",
      "현재 1074 -> 1075 click -> ..1075 크롤링 완료 / 누적 6750 건\n",
      "현재 1075 -> 1076 click -> ..1076 크롤링 완료 / 누적 6840 건\n",
      "현재 1076 -> 1077 click -> ..1077 크롤링 완료 / 누적 6930 건\n",
      "현재 1077 -> 1078 click -> ..1078 크롤링 완료 / 누적 7020 건\n",
      "현재 1078 -> 1079 click -> ..1079 크롤링 완료 / 누적 7110 건\n",
      "현재 1079 -> 1080 click -> ..1080 크롤링 완료 / 누적 7200 건\n",
      "현재 1080 -> 다음 click -> ..1081 크롤링 완료 / 누적 7290 건\n",
      "현재 1081 -> 1082 click -> ..1082 크롤링 완료 / 누적 7380 건\n",
      "현재 1082 -> 1083 click -> ..1083 크롤링 완료 / 누적 7470 건\n",
      "현재 1083 -> 1084 click -> ..1084 크롤링 완료 / 누적 7560 건\n",
      "현재 1084 -> 1085 click -> ..1085 크롤링 완료 / 누적 7650 건\n",
      "현재 1085 -> 1086 click -> ..1086 크롤링 완료 / 누적 7740 건\n",
      "현재 1086 -> 1087 click -> ..1087 크롤링 완료 / 누적 7830 건\n",
      "현재 1087 -> 1088 click -> ..1088 크롤링 완료 / 누적 7920 건\n",
      "현재 1088 -> 1089 click -> ..1089 크롤링 완료 / 누적 8010 건\n",
      "현재 1089 -> 1090 click -> ..1090 크롤링 완료 / 누적 8100 건\n",
      "현재 1090 -> 다음 click -> ..1091 크롤링 완료 / 누적 8190 건\n",
      "현재 1091 -> 1092 click -> ..1092 크롤링 완료 / 누적 8280 건\n",
      "현재 1092 -> 1093 click -> ..1093 크롤링 완료 / 누적 8370 건\n",
      "현재 1093 -> 1094 click -> ..1094 크롤링 완료 / 누적 8460 건\n",
      "현재 1094 -> 1095 click -> ..1095 크롤링 완료 / 누적 8550 건\n",
      "현재 1095 -> 1096 click -> ..1096 크롤링 완료 / 누적 8640 건\n",
      "현재 1096 -> 1097 click -> ..1097 크롤링 완료 / 누적 8730 건\n",
      "현재 1097 -> 1098 click -> ..1098 크롤링 완료 / 누적 8820 건\n",
      "현재 1098 -> 1099 click -> ..1099 크롤링 완료 / 누적 8910 건\n",
      "현재 1099 -> 1100 click -> ..1100 크롤링 완료 / 누적 9000 건\n",
      "현재 1100 -> 다음 click -> ..1101 크롤링 완료 / 누적 9090 건\n",
      "현재 1101 -> 1102 click -> ..1102 크롤링 완료 / 누적 9180 건\n",
      "현재 1102 -> 1103 click -> ..1103 크롤링 완료 / 누적 9270 건\n",
      "현재 1103 -> 1104 click -> ..1104 크롤링 완료 / 누적 9360 건\n",
      "현재 1104 -> 1105 click -> ..1105 크롤링 완료 / 누적 9450 건\n",
      "현재 1105 -> 1106 click -> ..1106 크롤링 완료 / 누적 9540 건\n",
      "현재 1106 -> 1107 click -> ..1107 크롤링 완료 / 누적 9630 건\n",
      "현재 1107 -> 1108 click -> ..1108 크롤링 완료 / 누적 9720 건\n",
      "현재 1108 -> 1109 click -> ..1109 크롤링 완료 / 누적 9810 건\n",
      "현재 1109 -> 1110 click -> ..1110 크롤링 완료 / 누적 9900 건\n",
      "현재 1110 -> 다음 click -> ..1111 크롤링 완료 / 누적 9990 건\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (9990, 2), indices imply (10080, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   1701\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_form_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1702\u001b[1;33m         \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1703\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_verify_integrity\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmgr_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconstruction_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtot_items\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (9990, 2), indices imply (10080, 2)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-6cb0c23f770c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'='\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[0mfileName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoday\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_page\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_page\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_page\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop_page\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m \u001b[0msaveFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfindPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_page\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpageSel\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop_page\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpageSel\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'예상 데이터 개수 :'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstop_page\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_page\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpageSel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'긁어온 데이터개수 :'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-68-6cb0c23f770c>\u001b[0m in \u001b[0;36msaveFile\u001b[1;34m(path, data, start_index, last_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;31m# 데이터 저장 : list -> df -> csv 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msaveFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[1;31m# 저장 결과 반환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    580\u001b[0m                             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mibase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m                     \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    583\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m                     \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   1704\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmgr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1705\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1706\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mconstruction_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (9990, 2), indices imply (10080, 2)"
     ]
    }
   ],
   "source": [
    "# 크롤링을 위해 사용자가 입력해야 되는 값\n",
    "start_page = 1000\n",
    "refresh_unit = 0\n",
    "stop_page = 1111\n",
    "file_name = 'patent_{date}_{first_page}_{last_page}.csv'\n",
    "\n",
    "# 메인\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "from math import ceil\n",
    "\n",
    "# 함수 : 딜레이\n",
    "def delay(text, sec):\n",
    "    print(text, end=\"\")\n",
    "    for i in range(sec): print('.', end=\"\"); time.sleep(1)\n",
    "\n",
    "# 함수 : 사이트에서 제목/내용 크롤링\n",
    "current_page_css_selector = 'span.board_pager03 strong'\n",
    "def contentCrawling(current_page_css_selector = current_page_css_selector):\n",
    "    dom = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    patent_list = dom.select('.search_section article')\n",
    "    \n",
    "    current_page = driver.find_element_by_css_selector(current_page_css_selector).text\n",
    "\n",
    "    result = [\n",
    "            {\n",
    "                '제목': patent.select_one('.search_section_title h1 > a:nth-of-type(2)').text.strip(),\n",
    "                '내용': patent.select_one('.search_txt').text.strip()\n",
    "            }\n",
    "            for patent in patent_list\n",
    "    ]\n",
    "\n",
    "    return int(current_page), result   # (크롤링한 페이지, 크롤링 결과) 반환\n",
    "\n",
    "# 함수 : 저장할 파일 경로 찾기\n",
    "def findPath(fileName):\n",
    "    file_path = r'D:\\DevRoot\\StockHelper\\dataset'\n",
    "    try: \n",
    "        path = os.path.join(file_path, fileName)\n",
    "    except(Exception):\n",
    "        file_path = 'C:\\DeepLearning_Project\\StockHelper\\StockHelper\\dataset'\n",
    "        path = os.path.join(file_path, fileName)\n",
    "        \n",
    "    return path\n",
    "\n",
    "# 함수 : 파일저장\n",
    "# 데이터 저장 : list -> df -> csv 저장\n",
    "def saveFile(path, data):\n",
    "    pd.DataFrame(data).to_csv(path, encoding='utf-8')\n",
    "    # 저장 결과 반환\n",
    "    if os.path.isfile(path):\n",
    "        print('>>> 파일변환 완료:', datetime.today().strftime((\"%Y-%m-%d %H:%M:%S\")))\n",
    "        print('>>> 저장위치:', path)\n",
    "    else: print('>>> 파일변환 실패')\n",
    "\n",
    "print('프로그램 시작')\n",
    "start = time.time()  # 시작 시간 저장\n",
    "result = []\n",
    "check_page_list = []   # 실제 페이지 이동 결과 (페이지 이동의 중복/누락 확인용)\n",
    "check_save_point = False   # 파일 저장하는 시점 확인\n",
    "\n",
    "# 1. webdriver를 이용해 kipris 접속\n",
    "driver_path = r'D:\\DevRoot\\download\\chromedriver.exe'\n",
    "try: \n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "except(Exception):\n",
    "    driver_path = r'C:\\DevRoot\\download\\chromedriver.exe'\n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "finally:\n",
    "    driver.implicitly_wait(10)   # 웹 페이지 로딩 완료 최대 대기 시간 (한번만 설정하면 driver를 사용하는 모든 코드에 적용)\n",
    "\n",
    "if refresh_unit <= 0: refresh_unit = 100000000\n",
    "for refreshed_num in range(1, ceil(stop_page / refresh_unit) + 1):\n",
    "    print('인터넷 접속중')\n",
    "    driver.get(\"http://kpat.kipris.or.kr/kpat/searchLogina.do?next=MainSearch\")\n",
    "    \n",
    "    # 크롤링 시작페이지 재설정\n",
    "    if refreshed_num != 1: start_page = refresh_unit * (refreshed_num - 1) + 1\n",
    "    \n",
    "    # 2. 검색 옵션 설정\n",
    "    # 2.1. 행정상태 변경\n",
    "    # defalut 해제 <- '전체' 클릭\n",
    "    driver.find_element_by_css_selector('form#leftside .release_list > span:nth-of-type(1) > input').click()\n",
    "    # 원하는 checkbox만 선택 <- '등록' 클릭\n",
    "    driver.find_element_by_css_selector('form#leftside .release_list > span:nth-last-of-type(1) > input').click()\n",
    "\n",
    "    # 2.2. 기간을 검색어로 입력\n",
    "    today = datetime.today().strftime(\"%Y%m%d\")\n",
    "    decade = str(int(today) - int('00001000'))\n",
    "    driver.find_element_by_css_selector('.keyword').send_keys(f'GD=[{decade}~{today}]')\n",
    "    driver.find_element_by_css_selector('.input_btn img').click()\n",
    "\n",
    "    # 2.3. 90개씩 보기 선택\n",
    "    pageSel = 90   # 페이지당 게시물 개수 (30, 60, 90 중 택1)\n",
    "    select = Select(driver.find_element_by_id('opt28'))\n",
    "    select.select_by_value(str(pageSel))\n",
    "    driver.find_element_by_css_selector('#pageSel img').click()\n",
    "\n",
    "    # 3. 데이터 추출\n",
    "    delay('크롤링 준비중', 3); print('완료')\n",
    "    page_num = 'span.board_pager03 a:nth-last-of-type({0})'   # target_page 구할 때 이용\n",
    "\n",
    "    # 3.1. 첫 페이지 크롤링\n",
    "    current_page, data = contentCrawling()\n",
    "    if current_page == stop_page: break   # 실행종료\n",
    "    if current_page != start_page:   # 첫 페이지 찾기\n",
    "        delay('시작 페이지로 이동', 0)\n",
    "        while current_page < start_page:\n",
    "            if current_page // 10 < start_page // 10:\n",
    "                driver.find_element_by_css_selector(page_num.format(1)).click()\n",
    "                delay('', 2)\n",
    "                current_page = int(driver.find_element_by_css_selector(current_page_css_selector).text)\n",
    "                continue\n",
    "            for i in range(10, 0, -1):\n",
    "                driver.find_element_by_css_selector(page_num.format(i)).click()\n",
    "                delay('', 2)\n",
    "                current_page = int(driver.find_element_by_css_selector(current_page_css_selector).text)\n",
    "                if current_page == start_page: print('완료'); break\n",
    "    current_page, data = contentCrawling()\n",
    "    delay('', 2)\n",
    "    result.extend(data)\n",
    "    print(f'{current_page} 위치 -> {current_page} 페이지 크롤링 완료 / 누적 {len(result)} 건')\n",
    "    check_page_list.append(current_page)\n",
    "\n",
    "    # 3.2. 페이지 이동하며 크롤링\n",
    "    while True:\n",
    "        # 실행 종료\n",
    "        if current_page >= stop_page: break\n",
    "        if current_page == refresh_unit * refreshed_num: break\n",
    "\n",
    "        for i in range(10, 0, -1):\n",
    "            if current_page >= stop_page: break\n",
    "            if current_page == refresh_unit * refreshed_num: check_save_point = True; break\n",
    "\n",
    "            # 3.2.1. 크롤링할 페이지(target_page)가 현재 페이지의 다음 페이지인 지 확인\n",
    "            target_page = driver.find_element_by_css_selector(page_num.format(i))\n",
    "            if i != 1 and int(target_page.text) <= current_page: delay('', 1); continue \n",
    "\n",
    "            print(f'현재 {current_page}', end=\" -> \")\n",
    "            dataLen1 = current_page * pageSel   # 현재 페이지의 크롤링 데이터 개수\n",
    "            # 3.2.2. 크롤링할 페이지(target_page)인 다음 페이지로 이동\n",
    "            try:\n",
    "                print(f'{target_page.text} click', end=\" -> \")\n",
    "            except Exception:\n",
    "                driver.refresh()   # 새로고침\n",
    "                dealy('', 5)\n",
    "                dom = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                patent_list = dom.select('.search_section article')\n",
    "            finally:\n",
    "                target_page.click()  # 크롤링할 페이지로 이동\n",
    "                delay('', 2)\n",
    "            while True:\n",
    "                try: # (클릭해서 이동한) 현재 페이지(click_page)는 target_page\n",
    "                    click_page = driver.find_element_by_css_selector(current_page_css_selector).text\n",
    "                except Exception:\n",
    "                    driver.refresh()   # 새로고침\n",
    "                    dom = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    patent_list = dom.select('.search_section article')\n",
    "                    click_page = int(driver.find_element_by_css_selector(current_page_css_selector).text)\n",
    "\n",
    "                if current_page != click_page: break  # 페이지 이동 시 loop 탈출\n",
    "            # 3.2.3. 크롤링\n",
    "            try:\n",
    "                current_page, data = contentCrawling()\n",
    "            except Exception:\n",
    "                driver.refresh()   # 새로고침\n",
    "                current_page, data = contentCrawling()\n",
    "            finally:\n",
    "                result.extend(data)\n",
    "            # 3.2.4. 중간결과 반환\n",
    "            dataLen2 = current_page * pageSel   # 현재 페이지의 크롤링 데이터 개수\n",
    "            print(f'{current_page} 크롤링 완료 / 누적 {len(result)} 건')\n",
    "            check_page_list.append(current_page)\n",
    "            \n",
    "    # 4.1. 파일 저장\n",
    "    if check_save_point:\n",
    "        print('=' * 60)\n",
    "        fileName = file_name.format(date=today, first_page=start_page, last_page=current_page)\n",
    "        saveFile(path=findPath(fileName), data=result, start_index=(start_page - 1) * pageSel + 1, last_index=current_page * pageSel + 1)\n",
    "        \n",
    "        # 4.2. 저장 결과\n",
    "        print('예상 데이터 개수 :', (current_page - start_page + 1) * pageSel)\n",
    "        print('긁어온 데이터개수 :', len(result))\n",
    "        normal_page_list = [i for i in range(start_page, current_page + 1)]   # 정상적으로 크롤링했을 때\n",
    "        num = len(normal_page_list)\n",
    "        for i in range(start_page, current_page + 1):\n",
    "            if i in check_page_list:\n",
    "                check_page_list.pop(check_page_list.index(i))\n",
    "                normal_page_list.pop(normal_page_list.index(i))\n",
    "        print('중복된 페이지 :', check_page_list if check_page_list else '없음')\n",
    "        print('누락된 페이지 :', normal_page_list if normal_page_list else '없음')\n",
    "        print('=' * 60)\n",
    "        check_page_list = []\n",
    "        normal_page_list = []\n",
    "        check_save_point = False\n",
    "        result = []\n",
    "        \n",
    "\n",
    "print('=' * 60)\n",
    "fileName = file_name.format(date=today, first_page=start_page, last_page=stop_page)\n",
    "saveFile(path=findPath(fileName), data=result, start_index=(start_page - 1) * pageSel + 1, last_index=stop_page * pageSel + 1)\n",
    "print('예상 데이터 개수 :', (stop_page - start_page + 1) * pageSel)\n",
    "print('긁어온 데이터개수 :', len(result))\n",
    "normal_page_list = [i for i in range(start_page, current_page + 1)]   # 정상적으로 크롤링했을 때\n",
    "num = len(normal_page_list)\n",
    "for i in range(start_page, current_page + 1):\n",
    "    if i in check_page_list:\n",
    "        check_page_list.pop(check_page_list.index(i))\n",
    "        normal_page_list.pop(normal_page_list.index(i))\n",
    "print('중복된 페이지 :', check_page_list if check_page_list else '없음')\n",
    "print('누락된 페이지 :', normal_page_list if normal_page_list else '없음')\n",
    "print('소요시간 :', int(time.time() - start) / 60, '분')  # 현재시각 - 시작시간 = 실행 시간\n",
    "print('프로그램 종료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "443bf821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 파일변환 완료: 2021-12-14 02:46:46\n",
      ">>> 저장위치: D:\\DevRoot\\StockHelper\\dataset\\patent_20211214_1000_1111.csv\n",
      "예상 데이터 개수 : 10080\n",
      "긁어온 데이터개수 : 9990\n",
      "중복된 페이지 : 없음\n",
      "누락된 페이지 : [1000]\n"
     ]
    }
   ],
   "source": [
    "def saveFile(path, data):\n",
    "    pd.DataFrame(data).to_csv(path, encoding='utf-8')\n",
    "    # 저장 결과 반환\n",
    "    if os.path.isfile(path):\n",
    "        print('>>> 파일변환 완료:', datetime.today().strftime((\"%Y-%m-%d %H:%M:%S\")))\n",
    "        print('>>> 저장위치:', path)\n",
    "    else: print('>>> 파일변환 실패')\n",
    "        \n",
    "fileName = file_name.format(date=today, first_page=start_page, last_page=stop_page)\n",
    "saveFile(path=findPath(fileName), data=result)\n",
    "print('예상 데이터 개수 :', (stop_page - start_page + 1) * pageSel)\n",
    "print('긁어온 데이터개수 :', len(result))\n",
    "normal_page_list = [i for i in range(start_page, current_page + 1)]   # 정상적으로 크롤링했을 때\n",
    "num = len(normal_page_list)\n",
    "for i in range(start_page, current_page + 1):\n",
    "    if i in check_page_list:\n",
    "        check_page_list.pop(check_page_list.index(i))\n",
    "        normal_page_list.pop(normal_page_list.index(i))\n",
    "print('중복된 페이지 :', check_page_list if check_page_list else '없음')\n",
    "print('누락된 페이지 :', normal_page_list if normal_page_list else '없음')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a22d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
