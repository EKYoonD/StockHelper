{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a9b5971",
   "metadata": {},
   "source": [
    "---\n",
    "## [이용 가이드]\n",
    "\n",
    "> `start_page` : 크롤링할 시작 페이지  \n",
    "> `resresh_unit` : 새로고침 단위  \n",
    "    - 320 페이지 이상 이동시 크롬 out of memory 발생  \n",
    "    - 0 이하 입력시 새로고침 없음  \n",
    "> `stop_page` : 크롤링할 마지막 페이지  \n",
    "> `file_name` : 저장할 파일명  \n",
    "- .기호 개당 1초를 의미 <- 사용자 정의 함수 delay(str, int)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3681e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링을 위해 사용자가 입력해야 되는 값\n",
    "start_page = 1001    # 크롤링할 시작 페이지\n",
    "refresh_unit = 0   # 크롤링 후 새로고침할 단위\n",
    "stop_page = 1111   # 크롤링할 마지막 페이지\n",
    "file_name = 'patent_{date}_{first_page}_{last_page}.csv'  # 저장할 파일명\n",
    "first_page = start_page\n",
    "\n",
    "# 메인\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "from math import ceil\n",
    "\n",
    "# 함수 : 딜레이\n",
    "def delay(text, sec):\n",
    "    print(text, end=\"\")\n",
    "    for i in range(sec): print('.', end=\"\"); time.sleep(1)\n",
    "\n",
    "# 함수 : 사이트에서 제목/내용 크롤링\n",
    "current_page_css_selector = 'span.board_pager03 strong'\n",
    "def contentCrawling(current_page_css_selector = current_page_css_selector):\n",
    "    dom = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    patent_list = dom.select('.search_section article')\n",
    "    \n",
    "    current_page = driver.find_element_by_css_selector(current_page_css_selector).text\n",
    "\n",
    "    result = [\n",
    "            {\n",
    "                '제목': patent.select_one('.search_section_title h1 > a:nth-of-type(2)').text.strip(),\n",
    "                '내용': patent.select_one('.search_txt').text.strip()\n",
    "            }\n",
    "            for patent in patent_list\n",
    "    ]\n",
    "\n",
    "    return int(current_page), result   # (크롤링한 페이지, 크롤링 결과) 반환\n",
    "\n",
    "# 함수 : 저장할 파일 경로 찾기\n",
    "def findPath(fileName):\n",
    "    file_path = r'D:\\DevRoot\\StockHelper\\dataset'\n",
    "    try: \n",
    "        path = os.path.join(file_path, fileName)\n",
    "    except(Exception):\n",
    "        file_path = 'C:\\DeepLearning_Project\\StockHelper\\StockHelper\\dataset'\n",
    "        path = os.path.join(file_path, fileName)\n",
    "        \n",
    "    return path\n",
    "\n",
    "print('프로그램 시작')\n",
    "start = time.time()  # 시작 시간 저장\n",
    "result = []\n",
    "check_page_list = []   # 실제 페이지 이동 결과 (페이지 이동의 중복/누락 확인용)\n",
    "\n",
    "# 1. webdriver를 이용해 kipris 접속\n",
    "driver_path = r'D:\\DevRoot\\download\\chromedriver.exe'\n",
    "try: \n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "except(Exception):\n",
    "    driver_path = r'C:\\DevRoot\\download\\chromedriver.exe'\n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "finally:\n",
    "    driver.implicitly_wait(10)   # 웹 페이지 로딩 완료 최대 대기 시간 (한번만 설정하면 driver를 사용하는 모든 코드에 적용)\n",
    "\n",
    "if refresh_unit <= 0: refresh_unit = 100000000\n",
    "for refreshed_num in range(1, ceil(stop_page / refresh_unit) + 1):\n",
    "    print('인터넷 접속중')\n",
    "    driver.get(\"http://kpat.kipris.or.kr/kpat/searchLogina.do?next=MainSearch\")\n",
    "    \n",
    "    # 크롤링 시작페이지 재설정\n",
    "    if refreshed_num != 1: start_page = refresh_unit * (refreshed_num - 1) + 1\n",
    "    \n",
    "    # 2. 검색 옵션 설정\n",
    "    # 2.1. 행정상태 변경\n",
    "    # defalut 해제 <- '전체' 클릭\n",
    "    driver.find_element_by_css_selector('form#leftside .release_list > span:nth-of-type(1) > input').click()\n",
    "    # 원하는 checkbox만 선택 <- '등록' 클릭\n",
    "    driver.find_element_by_css_selector('form#leftside .release_list > span:nth-last-of-type(1) > input').click()\n",
    "\n",
    "    # 2.2. 기간을 검색어로 입력\n",
    "    today = datetime.today().strftime(\"%Y%m%d\")\n",
    "    decade = str(int(today) - int('00001000'))\n",
    "    driver.find_element_by_css_selector('.keyword').send_keys(f'GD=[{decade}~{today}]')\n",
    "    driver.find_element_by_css_selector('.input_btn img').click()\n",
    "\n",
    "    # 2.3. 90개씩 보기 선택\n",
    "    pageSel = 90   # 페이지당 게시물 개수 (30, 60, 90 중 택1)\n",
    "    select = Select(driver.find_element_by_id('opt28'))\n",
    "    select.select_by_value(str(pageSel))\n",
    "    driver.find_element_by_css_selector('#pageSel img').click()\n",
    "\n",
    "    # 3. 데이터 추출\n",
    "    delay('크롤링 준비중', 3); print('완료')\n",
    "    page_num = 'span.board_pager03 a:nth-last-of-type({0})'   # target_page 구할 때 이용\n",
    "\n",
    "    # 3.1. 첫 페이지 크롤링\n",
    "    current_page, data = contentCrawling()\n",
    "    if current_page == stop_page: break   # 실행종료\n",
    "    if current_page != start_page:   # 첫 페이지 찾기\n",
    "        delay('시작 페이지로 이동', 0)\n",
    "        while current_page < start_page:\n",
    "            if current_page // 10 < start_page // 10:\n",
    "                driver.find_element_by_css_selector(page_num.format(1)).click()\n",
    "                delay('', 2)\n",
    "                current_page = int(driver.find_element_by_css_selector(current_page_css_selector).text)\n",
    "                continue\n",
    "            for i in range(10, 0, -1):\n",
    "                driver.find_element_by_css_selector(page_num.format(i)).click()\n",
    "                delay('', 2)\n",
    "                current_page = int(driver.find_element_by_css_selector(current_page_css_selector).text)\n",
    "                if current_page == start_page: print('완료'); break\n",
    "    current_page, data = contentCrawling()\n",
    "    delay('', 2)\n",
    "    result.extend(data)\n",
    "    print(f'{current_page} 위치 -> {current_page} 페이지 크롤링 완료 / 누적 데이터 {len(result)} 건')\n",
    "    check_page_list.append(current_page)\n",
    "\n",
    "    # 3.2. 페이지 이동하며 크롤링\n",
    "    while True:\n",
    "        # 실행 종료\n",
    "        if current_page >= stop_page: break\n",
    "        if current_page == refresh_unit * refreshed_num: break\n",
    "\n",
    "        for i in range(10, 0, -1):\n",
    "            if current_page >= stop_page: break\n",
    "            if current_page == refresh_unit * refreshed_num: break\n",
    "\n",
    "            # 3.2.1. 크롤링할 페이지(target_page)가 현재 페이지의 다음 페이지인 지 확인\n",
    "            target_page = driver.find_element_by_css_selector(page_num.format(i))\n",
    "            if i != 1 and int(target_page.text) <= current_page: delay('', 1); continue \n",
    "\n",
    "            print(f'현재 {current_page}', end=\" -> \")\n",
    "            # 3.2.2. 크롤링할 페이지(target_page)인 다음 페이지로 이동\n",
    "            try:\n",
    "                print(f'{target_page.text} click', end=\" -> \")\n",
    "            except Exception:\n",
    "                driver.refresh()   # 새로고침\n",
    "                dealy('', 5)\n",
    "                dom = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                patent_list = dom.select('.search_section article')\n",
    "            finally:\n",
    "                target_page.click()  # 크롤링할 페이지로 이동\n",
    "                delay('', 2)\n",
    "            while True:\n",
    "                try: # (클릭해서 이동한) 현재 페이지(click_page)는 target_page\n",
    "                    click_page = driver.find_element_by_css_selector(current_page_css_selector).text\n",
    "    #             except KeyboardInterrupt or WebDriverException:\n",
    "    #                 print('Interrupted')\n",
    "    #                 try: sys.exit(0)\n",
    "    #                 except SystemExit: os._exit(0)\n",
    "    #                 finally:\n",
    "    #                     print('긁어온 데이터개수 :', len(result))\n",
    "    #                     print('소요시간 :', int(time.time() - start) / 60, '분')  # 현재시각 - 시작시간 = 실행 시간\n",
    "    #                     print('프로그램 종료')\n",
    "                except Exception:\n",
    "                    driver.refresh()   # 새로고침\n",
    "                    dealy('', 4)\n",
    "                    dom = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    patent_list = dom.select('.search_section article')\n",
    "                    click_page = int(driver.find_element_by_css_selector(current_page_css_selector).text)\n",
    "\n",
    "                if current_page != click_page: break  # 페이지 이동 시 loop 탈출\n",
    "            # 3.2.3. 크롤링\n",
    "            try:\n",
    "                current_page, data = contentCrawling()\n",
    "            except Exception:\n",
    "                driver.refresh()   # 새로고침\n",
    "                dealy('', 5)\n",
    "                current_page, data = contentCrawling()\n",
    "            finally:\n",
    "                result.extend(data)\n",
    "            # 3.2.4. 중간결과 반환\n",
    "            print(f'{current_page} 페이지 크롤링 완료 / 누적 데이터 {len(result)} 건')\n",
    "            check_page_list.append(current_page)\n",
    "        \n",
    "print('=' * 60)\n",
    "print('예상 데이터 개수 :', (stop_page - start_page + 1) * pageSel)\n",
    "print('긁어온 데이터 개수 :', len(result))\n",
    "normal_page_list = [i for i in range(start_page, current_page + 1)]   # 정상적으로 크롤링했을 때\n",
    "num = len(normal_page_list)\n",
    "for i in range(start_page, current_page + 1):\n",
    "    if i in check_page_list:\n",
    "        check_page_list.pop(check_page_list.index(i))\n",
    "        normal_page_list.pop(normal_page_list.index(i))\n",
    "print('중복된 페이지 :', check_page_list if check_page_list else '없음')\n",
    "print('누락된 페이지 :', normal_page_list if normal_page_list else '없음')\n",
    "print('소요시간 :', int(time.time() - start) / 60, '분')  # 현재시각 - 시작시간 = 실행 시간\n",
    "\n",
    "# 데이터 저장 : list -> df -> csv 저장\n",
    "fileName = file_name.format(date=today, first_page=first_page, last_page=stop_page)\n",
    "path = findPath(fileName)\n",
    "start_index = (start_page - 1) * pageSel\n",
    "last_index = stop_page * pageSel\n",
    "pd.DataFrame(result, index=range(start_index, last_index)).to_csv(path, encoding='utf-8')\n",
    "\n",
    "# 저장 결과 반환\n",
    "if os.path.isfile(path):\n",
    "    print('>>> 파일변환 완료:', datetime.today().strftime((\"%Y-%m-%d %H:%M:%S\")))\n",
    "    print('>>> 저장위치:', path)\n",
    "else: print('>>> 파일변환 실패')\n",
    "    \n",
    "print('프로그램 종료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869cfd0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
