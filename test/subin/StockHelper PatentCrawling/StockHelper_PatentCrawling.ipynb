{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25beff90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프로그램 시작\n",
      "인터넷 접속중\n",
      "크롤링 준비중...완료\n",
      "..1 위치 -> 1 페이지 크롤링 완료 / 누적 90 건\n",
      "현재 1 -> 2 click -> ..2 크롤링 완료 / 누적 180 건\n",
      "현재 2 -> 3 click -> ..3 크롤링 완료 / 누적 270 건\n",
      "현재 3 -> 4 click -> ..4 크롤링 완료 / 누적 360 건\n",
      "현재 4 -> 5 click -> ..5 크롤링 완료 / 누적 450 건\n",
      "현재 5 -> 6 click -> ..6 크롤링 완료 / 누적 540 건\n",
      "현재 6 -> 7 click -> ..7 크롤링 완료 / 누적 630 건\n",
      "현재 7 -> 8 click -> ..8 크롤링 완료 / 누적 720 건\n",
      "현재 8 -> 9 click -> ..9 크롤링 완료 / 누적 810 건\n",
      "현재 9 -> 10 click -> ..10 크롤링 완료 / 누적 900 건\n",
      "현재 10 -> 다음 click -> ..11 크롤링 완료 / 누적 990 건\n",
      "현재 11 -> 12 click -> ..12 크롤링 완료 / 누적 1080 건\n",
      "현재 12 -> 13 click -> ..13 크롤링 완료 / 누적 1170 건\n",
      "현재 13 -> 14 click -> ..14 크롤링 완료 / 누적 1260 건\n",
      "현재 14 -> 15 click -> ..15 크롤링 완료 / 누적 1350 건\n",
      "현재 15 -> 16 click -> ..16 크롤링 완료 / 누적 1440 건\n",
      "현재 16 -> 17 click -> ..17 크롤링 완료 / 누적 1530 건\n",
      "현재 17 -> 18 click -> ..18 크롤링 완료 / 누적 1620 건\n",
      "현재 18 -> 19 click -> ..19 크롤링 완료 / 누적 1710 건\n",
      "현재 19 -> 20 click -> ..20 크롤링 완료 / 누적 1800 건\n",
      "현재 20 -> 다음 click -> ..21 크롤링 완료 / 누적 1890 건\n",
      "현재 21 -> 22 click -> ..22 크롤링 완료 / 누적 1980 건\n",
      "현재 22 -> 23 click -> ..23 크롤링 완료 / 누적 2070 건\n",
      "현재 23 -> 24 click -> ..24 크롤링 완료 / 누적 2160 건\n",
      "현재 24 -> 25 click -> ..25 크롤링 완료 / 누적 2250 건\n",
      "현재 25 -> 26 click -> ..26 크롤링 완료 / 누적 2340 건\n",
      "현재 26 -> 27 click -> ..27 크롤링 완료 / 누적 2430 건\n",
      "현재 27 -> 28 click -> ..28 크롤링 완료 / 누적 2520 건\n",
      "현재 28 -> 29 click -> ..29 크롤링 완료 / 누적 2610 건\n",
      "현재 29 -> 30 click -> ..30 크롤링 완료 / 누적 2700 건\n",
      "현재 30 -> 다음 click -> ..31 크롤링 완료 / 누적 2790 건\n",
      "현재 31 -> 32 click -> ..32 크롤링 완료 / 누적 2880 건\n",
      "현재 32 -> 33 click -> ..33 크롤링 완료 / 누적 2970 건\n",
      "현재 33 -> 34 click -> ..34 크롤링 완료 / 누적 3060 건\n",
      "현재 34 -> 35 click -> ..35 크롤링 완료 / 누적 3150 건\n",
      "현재 35 -> 36 click -> ..36 크롤링 완료 / 누적 3240 건\n",
      "현재 36 -> 37 click -> ..37 크롤링 완료 / 누적 3330 건\n",
      "현재 37 -> 38 click -> ..38 크롤링 완료 / 누적 3420 건\n",
      "현재 38 -> 39 click -> ..39 크롤링 완료 / 누적 3510 건\n",
      "현재 39 -> 40 click -> ..40 크롤링 완료 / 누적 3600 건\n",
      "현재 40 -> 다음 click -> ..41 크롤링 완료 / 누적 3690 건\n",
      "현재 41 -> 42 click -> ..42 크롤링 완료 / 누적 3780 건\n",
      "현재 42 -> 43 click -> ..43 크롤링 완료 / 누적 3870 건\n",
      "현재 43 -> 44 click -> ..44 크롤링 완료 / 누적 3960 건\n",
      "현재 44 -> 45 click -> ..45 크롤링 완료 / 누적 4050 건\n",
      "현재 45 -> 46 click -> ..46 크롤링 완료 / 누적 4140 건\n",
      "현재 46 -> 47 click -> ..47 크롤링 완료 / 누적 4230 건\n",
      "현재 47 -> 48 click -> ..48 크롤링 완료 / 누적 4320 건\n",
      "현재 48 -> 49 click -> ..49 크롤링 완료 / 누적 4410 건\n",
      "현재 49 -> 50 click -> ..50 크롤링 완료 / 누적 4500 건\n",
      "현재 50 -> 다음 click -> ..50 크롤링 완료 / 누적 4590 건\n",
      ".........현재 50 -> 다음 click -> ..61 크롤링 완료 / 누적 4680 건\n",
      "현재 61 -> 62 click -> ..62 크롤링 완료 / 누적 4770 건\n",
      "현재 62 -> 63 click -> ..63 크롤링 완료 / 누적 4860 건\n",
      "현재 63 -> 64 click -> ..64 크롤링 완료 / 누적 4950 건\n",
      "현재 64 -> 65 click -> ..65 크롤링 완료 / 누적 5040 건\n",
      "현재 65 -> 66 click -> ..66 크롤링 완료 / 누적 5130 건\n",
      "현재 66 -> 67 click -> ..67 크롤링 완료 / 누적 5220 건\n",
      "현재 67 -> 68 click -> ..68 크롤링 완료 / 누적 5310 건\n",
      "현재 68 -> 69 click -> ..69 크롤링 완료 / 누적 5400 건\n",
      "현재 69 -> 70 click -> ..70 크롤링 완료 / 누적 5490 건\n",
      "현재 70 -> 다음 click -> ..71 크롤링 완료 / 누적 5580 건\n",
      "현재 71 -> 72 click -> ..72 크롤링 완료 / 누적 5670 건\n",
      "현재 72 -> 73 click -> ..73 크롤링 완료 / 누적 5760 건\n",
      "현재 73 -> 74 click -> ..74 크롤링 완료 / 누적 5850 건\n",
      "현재 74 -> 75 click -> ..75 크롤링 완료 / 누적 5940 건\n",
      "현재 75 -> 76 click -> ..76 크롤링 완료 / 누적 6030 건\n",
      "현재 76 -> 77 click -> ..77 크롤링 완료 / 누적 6120 건\n",
      "현재 77 -> 78 click -> ..78 크롤링 완료 / 누적 6210 건\n",
      "현재 78 -> 79 click -> ..79 크롤링 완료 / 누적 6300 건\n",
      "현재 79 -> 80 click -> ..80 크롤링 완료 / 누적 6390 건\n",
      "현재 80 -> 다음 click -> ..81 크롤링 완료 / 누적 6480 건\n",
      "현재 81 -> 82 click -> ..82 크롤링 완료 / 누적 6570 건\n",
      "현재 82 -> 83 click -> ..83 크롤링 완료 / 누적 6660 건\n",
      "현재 83 -> 84 click -> ..84 크롤링 완료 / 누적 6750 건\n",
      "현재 84 -> 85 click -> ..85 크롤링 완료 / 누적 6840 건\n",
      "현재 85 -> 86 click -> ..86 크롤링 완료 / 누적 6930 건\n",
      "현재 86 -> 87 click -> ..87 크롤링 완료 / 누적 7020 건\n",
      "현재 87 -> 88 click -> ..88 크롤링 완료 / 누적 7110 건\n",
      "현재 88 -> 89 click -> ..89 크롤링 완료 / 누적 7200 건\n",
      "현재 89 -> 90 click -> ..89 크롤링 완료 / 누적 7290 건\n",
      "현재 89 -> 다음 click -> ..89 크롤링 완료 / 누적 7380 건\n",
      ".현재 89 -> 93 click -> ..93 크롤링 완료 / 누적 7470 건\n",
      "현재 93 -> 94 click -> ..94 크롤링 완료 / 누적 7560 건\n",
      "현재 94 -> 95 click -> ..95 크롤링 완료 / 누적 7650 건\n",
      "현재 95 -> 96 click -> ..96 크롤링 완료 / 누적 7740 건\n",
      "현재 96 -> 97 click -> ..97 크롤링 완료 / 누적 7830 건\n",
      "현재 97 -> 98 click -> ..98 크롤링 완료 / 누적 7920 건\n",
      "현재 98 -> 99 click -> ..99 크롤링 완료 / 누적 8010 건\n",
      "현재 99 -> 100 click -> ..100 크롤링 완료 / 누적 8100 건\n",
      "현재 100 -> 다음 click -> ..101 크롤링 완료 / 누적 8190 건\n",
      "현재 101 -> 102 click -> ..102 크롤링 완료 / 누적 8280 건\n",
      "현재 102 -> 103 click -> ..103 크롤링 완료 / 누적 8370 건\n",
      "현재 103 -> 104 click -> ..104 크롤링 완료 / 누적 8460 건\n",
      "현재 104 -> 105 click -> ..105 크롤링 완료 / 누적 8550 건\n",
      "현재 105 -> 106 click -> ..106 크롤링 완료 / 누적 8640 건\n",
      "현재 106 -> 107 click -> ..107 크롤링 완료 / 누적 8730 건\n",
      "현재 107 -> 108 click -> ..108 크롤링 완료 / 누적 8820 건\n",
      "현재 108 -> 109 click -> ..109 크롤링 완료 / 누적 8910 건\n",
      "현재 109 -> 110 click -> ..110 크롤링 완료 / 누적 9000 건\n",
      "현재 110 -> 다음 click -> ..111 크롤링 완료 / 누적 9090 건\n",
      "현재 111 -> 112 click -> ..112 크롤링 완료 / 누적 9180 건\n",
      "현재 112 -> 113 click -> ..113 크롤링 완료 / 누적 9270 건\n",
      "현재 113 -> 114 click -> ..114 크롤링 완료 / 누적 9360 건\n",
      "현재 114 -> 115 click -> ..115 크롤링 완료 / 누적 9450 건\n",
      "현재 115 -> 116 click -> ..116 크롤링 완료 / 누적 9540 건\n",
      "현재 116 -> 117 click -> ..117 크롤링 완료 / 누적 9630 건\n",
      "현재 117 -> 118 click -> ..118 크롤링 완료 / 누적 9720 건\n",
      "현재 118 -> 119 click -> ..119 크롤링 완료 / 누적 9810 건\n",
      "현재 119 -> 120 click -> ..120 크롤링 완료 / 누적 9900 건\n",
      "현재 120 -> 다음 click -> ..121 크롤링 완료 / 누적 9990 건\n",
      "현재 121 -> 122 click -> ..122 크롤링 완료 / 누적 10080 건\n",
      "현재 122 -> 123 click -> ..123 크롤링 완료 / 누적 10170 건\n",
      "현재 123 -> 124 click -> ..124 크롤링 완료 / 누적 10260 건\n",
      "현재 124 -> 125 click -> ..125 크롤링 완료 / 누적 10350 건\n",
      "현재 125 -> 126 click -> ..126 크롤링 완료 / 누적 10440 건\n",
      "현재 126 -> 127 click -> ..127 크롤링 완료 / 누적 10530 건\n",
      "현재 127 -> 128 click -> ..128 크롤링 완료 / 누적 10620 건\n",
      "현재 128 -> 129 click -> ..129 크롤링 완료 / 누적 10710 건\n",
      "현재 129 -> 130 click -> ..130 크롤링 완료 / 누적 10800 건\n",
      "현재 130 -> 다음 click -> ..131 크롤링 완료 / 누적 10890 건\n",
      "현재 131 -> 132 click -> ..132 크롤링 완료 / 누적 10980 건\n",
      "현재 132 -> 133 click -> ..133 크롤링 완료 / 누적 11070 건\n",
      "현재 133 -> 134 click -> ..134 크롤링 완료 / 누적 11160 건\n",
      "현재 134 -> 135 click -> ..135 크롤링 완료 / 누적 11250 건\n",
      "현재 135 -> 136 click -> ..136 크롤링 완료 / 누적 11340 건\n",
      "현재 136 -> 137 click -> ..137 크롤링 완료 / 누적 11430 건\n",
      "현재 137 -> 138 click -> ..138 크롤링 완료 / 누적 11520 건\n",
      "현재 138 -> 139 click -> ..139 크롤링 완료 / 누적 11610 건\n",
      "현재 139 -> 140 click -> ..140 크롤링 완료 / 누적 11700 건\n",
      "현재 140 -> 다음 click -> ..141 크롤링 완료 / 누적 11790 건\n",
      "현재 141 -> 142 click -> ..142 크롤링 완료 / 누적 11880 건\n",
      "현재 142 -> 143 click -> ..143 크롤링 완료 / 누적 11970 건\n",
      "현재 143 -> 144 click -> ..144 크롤링 완료 / 누적 12060 건\n",
      "현재 144 -> 145 click -> ..145 크롤링 완료 / 누적 12150 건\n",
      "현재 145 -> 146 click -> ..146 크롤링 완료 / 누적 12240 건\n",
      "현재 146 -> 147 click -> ..147 크롤링 완료 / 누적 12330 건\n",
      "현재 147 -> 148 click -> ..148 크롤링 완료 / 누적 12420 건\n",
      "현재 148 -> 149 click -> ..149 크롤링 완료 / 누적 12510 건\n",
      "현재 149 -> 150 click -> ..150 크롤링 완료 / 누적 12600 건\n",
      "현재 150 -> 다음 click -> ..151 크롤링 완료 / 누적 12690 건\n",
      "현재 151 -> 152 click -> ..152 크롤링 완료 / 누적 12780 건\n",
      "현재 152 -> 153 click -> ..153 크롤링 완료 / 누적 12870 건\n",
      "현재 153 -> 154 click -> ..154 크롤링 완료 / 누적 12960 건\n",
      "현재 154 -> 155 click -> ..155 크롤링 완료 / 누적 13050 건\n",
      "현재 155 -> 156 click -> ..155 크롤링 완료 / 누적 13140 건\n",
      "현재 155 -> 157 click -> ..157 크롤링 완료 / 누적 13230 건\n",
      "현재 157 -> 158 click -> ..157 크롤링 완료 / 누적 13320 건\n",
      "현재 157 -> 159 click -> ..157 크롤링 완료 / 누적 13410 건\n",
      "현재 157 -> 160 click -> ..158 크롤링 완료 / 누적 13500 건\n",
      "현재 158 -> 다음 click -> ..161 크롤링 완료 / 누적 13590 건\n",
      "현재 161 -> 162 click -> ..162 크롤링 완료 / 누적 13680 건\n",
      "현재 162 -> 163 click -> ..163 크롤링 완료 / 누적 13770 건\n",
      "현재 163 -> 164 click -> ..164 크롤링 완료 / 누적 13860 건\n",
      "현재 164 -> 165 click -> ..165 크롤링 완료 / 누적 13950 건\n",
      "현재 165 -> 166 click -> ..166 크롤링 완료 / 누적 14040 건\n",
      "현재 166 -> 167 click -> ..167 크롤링 완료 / 누적 14130 건\n",
      "현재 167 -> 168 click -> ..168 크롤링 완료 / 누적 14220 건\n",
      "현재 168 -> 169 click -> ..169 크롤링 완료 / 누적 14310 건\n",
      "현재 169 -> 170 click -> ..170 크롤링 완료 / 누적 14400 건\n",
      "현재 170 -> 다음 click -> ..171 크롤링 완료 / 누적 14490 건\n",
      "현재 171 -> 172 click -> ..172 크롤링 완료 / 누적 14580 건\n",
      "현재 172 -> 173 click -> ..173 크롤링 완료 / 누적 14670 건\n",
      "현재 173 -> 174 click -> ..174 크롤링 완료 / 누적 14760 건\n",
      "현재 174 -> 175 click -> ..175 크롤링 완료 / 누적 14850 건\n",
      "현재 175 -> 176 click -> ..176 크롤링 완료 / 누적 14940 건\n",
      "현재 176 -> 177 click -> ..177 크롤링 완료 / 누적 15030 건\n",
      "현재 177 -> 178 click -> ..178 크롤링 완료 / 누적 15120 건\n",
      "현재 178 -> 179 click -> ..179 크롤링 완료 / 누적 15210 건\n",
      "현재 179 -> 180 click -> ..180 크롤링 완료 / 누적 15300 건\n",
      "현재 180 -> 다음 click -> ..181 크롤링 완료 / 누적 15390 건\n",
      "현재 181 -> 182 click -> ..182 크롤링 완료 / 누적 15480 건\n",
      "현재 182 -> 183 click -> ..183 크롤링 완료 / 누적 15570 건\n",
      "현재 183 -> 184 click -> ..184 크롤링 완료 / 누적 15660 건\n",
      "현재 184 -> 185 click -> ..185 크롤링 완료 / 누적 15750 건\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 185 -> 186 click -> ..186 크롤링 완료 / 누적 15840 건\n",
      "현재 186 -> 187 click -> ..187 크롤링 완료 / 누적 15930 건\n",
      "현재 187 -> 188 click -> ..188 크롤링 완료 / 누적 16020 건\n",
      "현재 188 -> 189 click -> ..189 크롤링 완료 / 누적 16110 건\n",
      "현재 189 -> 190 click -> ..190 크롤링 완료 / 누적 16200 건\n",
      "현재 190 -> 다음 click -> ..191 크롤링 완료 / 누적 16290 건\n",
      "현재 191 -> 192 click -> ..192 크롤링 완료 / 누적 16380 건\n",
      "현재 192 -> 193 click -> ..193 크롤링 완료 / 누적 16470 건\n",
      "현재 193 -> 194 click -> ..194 크롤링 완료 / 누적 16560 건\n",
      "현재 194 -> 195 click -> ..195 크롤링 완료 / 누적 16650 건\n",
      "현재 195 -> 196 click -> ..196 크롤링 완료 / 누적 16740 건\n",
      "현재 196 -> 197 click -> ..197 크롤링 완료 / 누적 16830 건\n",
      "현재 197 -> 198 click -> ..198 크롤링 완료 / 누적 16920 건\n",
      "현재 198 -> 199 click -> ..199 크롤링 완료 / 누적 17010 건\n",
      "현재 199 -> 200 click -> ..200 크롤링 완료 / 누적 17100 건\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (17100, 2), indices imply (18000, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   1701\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_form_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1702\u001b[1;33m         \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1703\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_verify_integrity\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmgr_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconstruction_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtot_items\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (17100, 2), indices imply (18000, 2)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e3a7b60ace88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;31m# [참고] cmd pyinstaller 모듈 : https://dvlp-jun.tistory.com/26\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;31m# 매일 특정시간에 동작\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m \u001b[0mschedule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"21:50\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_page\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefresh_unit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_page\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_auto_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0mschedule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-e3a7b60ace88>\u001b[0m in \u001b[0;36mjob\u001b[1;34m(start_page, refresh_unit, stop_page, check_auto_save, file_name)\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'='\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[0mfileName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoday\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_page\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_page\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_page\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             \u001b[0msaveFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfindPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_page\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpageSel\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpageSel\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[1;31m# 4.2. 저장 결과\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-e3a7b60ace88>\u001b[0m in \u001b[0;36msaveFile\u001b[1;34m(path, data, start_index, last_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;31m# 데이터 저장 : list -> df -> csv 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msaveFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;31m# 저장 결과 반환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    580\u001b[0m                             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mibase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m                     \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    583\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m                     \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   1704\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmgr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1705\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1706\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mconstruction_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (17100, 2), indices imply (18000, 2)"
     ]
    }
   ],
   "source": [
    "# 크롤링을 위해 사용자가 입력해야 되는 값\n",
    "start_page = 1\n",
    "refresh_unit = 200\n",
    "stop_page = 1111\n",
    "check_auto_save = True\n",
    "file_name = 'patent_{date}_{first_page}_{last_page}.csv'\n",
    "firstPage = start_page   # 값 변경하면 안 됨\n",
    "\n",
    "# 메인\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "from math import ceil\n",
    "import schedule\n",
    "\n",
    "# 함수 : 딜레이\n",
    "def delay(text, sec):\n",
    "    print(text, end=\"\")\n",
    "    for i in range(sec): print('.', end=\"\"); time.sleep(1)\n",
    "\n",
    "# 함수 : 사이트에서 제목/내용 크롤링\n",
    "current_page_css_selector = 'span.board_pager03 strong'\n",
    "def contentCrawling(current_page_css_selector = current_page_css_selector):\n",
    "    dom = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    patent_list = dom.select('.search_section article')\n",
    "    \n",
    "    current_page = driver.find_element_by_css_selector(current_page_css_selector).text\n",
    "\n",
    "    result = [\n",
    "            {\n",
    "                '제목': patent.select_one('.search_section_title h1 > a:nth-of-type(2)').text.strip(),\n",
    "                '내용': patent.select_one('.search_txt').text.strip()\n",
    "            }\n",
    "            for patent in patent_list\n",
    "    ]\n",
    "\n",
    "    return int(current_page), result   # (크롤링한 페이지, 크롤링 결과) 반환\n",
    "\n",
    "# 함수 : 저장할 파일 경로 찾기\n",
    "def findPath(fileName):\n",
    "    file_path = r'D:\\DevRoot\\StockHelper\\dataset'\n",
    "    try: \n",
    "        path = os.path.join(file_path, fileName)\n",
    "    except(Exception):\n",
    "        file_path = 'C:\\DeepLearning_Project\\StockHelper\\StockHelper\\dataset'\n",
    "        path = os.path.join(file_path, fileName)\n",
    "        \n",
    "    return path\n",
    "\n",
    "# 함수 : 파일저장\n",
    "# 데이터 저장 : list -> df -> csv 저장\n",
    "def saveFile(path, data, start_index, last_index):\n",
    "    pd.DataFrame(data, index=range(start_index, last_index)).to_csv(path, encoding='utf-8')\n",
    "    # 저장 결과 반환\n",
    "    if os.path.isfile(path):\n",
    "        print('>>> 파일변환 완료:', datetime.today().strftime((\"%Y-%m-%d %H:%M:%S\")))\n",
    "        print('>>> 저장위치:', path)\n",
    "    else: print('>>> 파일변환 실패')\n",
    "        \n",
    "def job(start_page, refresh_unit, stop_page, check_auto_save, file_name):\n",
    "    print('프로그램 시작')\n",
    "    start = time.time()  # 프로그램 작동 시작 시간\n",
    "    result = []\n",
    "    check_page_list = []   # 실제 페이지 이동 결과 (페이지 이동의 중복/누락 확인용)\n",
    "    check_save_point = False   # 파일 저장하는 시점 확인\n",
    "\n",
    "    # 1. webdriver를 이용해 kipris 접속\n",
    "    driver_path = r'D:\\DevRoot\\download\\chromedriver.exe'\n",
    "    global driver\n",
    "    try: \n",
    "        driver = webdriver.Chrome(driver_path)\n",
    "    except(Exception):\n",
    "        driver_path = r'C:\\DevRoot\\download\\chromedriver.exe'\n",
    "        driver = webdriver.Chrome(driver_path)\n",
    "    finally:\n",
    "        driver.implicitly_wait(10)   # 웹 페이지 로딩 완료 최대 대기 시간 (한번만 설정하면 driver를 사용하는 모든 코드에 적용)\n",
    "\n",
    "    if refresh_unit <= 0: refresh_unit = 100000000\n",
    "    for refreshed_num in range(1, ceil(stop_page / refresh_unit) + 1):\n",
    "        print('인터넷 접속중')\n",
    "        driver.get(\"http://kpat.kipris.or.kr/kpat/searchLogina.do?next=MainSearch\")\n",
    "\n",
    "        # 크롤링 시작페이지 재설정\n",
    "        if refreshed_num != 1: start_page = refresh_unit * (refreshed_num - 1) + 1\n",
    "\n",
    "        # 2. 검색 옵션 설정\n",
    "        # 2.1. 행정상태 변경\n",
    "        # defalut 해제 <- '전체' 클릭\n",
    "        driver.find_element_by_css_selector('form#leftside .release_list > span:nth-of-type(1) > input').click()\n",
    "        # 원하는 checkbox만 선택 <- '등록' 클릭\n",
    "        driver.find_element_by_css_selector('form#leftside .release_list > span:nth-last-of-type(1) > input').click()\n",
    "\n",
    "        # 2.2. 기간을 검색어로 입력\n",
    "        today = datetime.today().strftime(\"%Y%m%d\")\n",
    "        decade = str(int(today) - int('00001000'))\n",
    "        driver.find_element_by_css_selector('.keyword').send_keys(f'GD=[{decade}~{today}]')\n",
    "        driver.find_element_by_css_selector('.input_btn img').click()\n",
    "\n",
    "        # 2.3. 90개씩 보기 선택\n",
    "        pageSel = 90   # 페이지당 게시물 개수 (30, 60, 90 중 택1)\n",
    "        select = Select(driver.find_element_by_id('opt28'))\n",
    "        select.select_by_value(str(pageSel))\n",
    "        driver.find_element_by_css_selector('#pageSel img').click()\n",
    "\n",
    "        # 3. 데이터 추출\n",
    "        delay('크롤링 준비중', 3); print('완료')\n",
    "        page_num = 'span.board_pager03 a:nth-last-of-type({0})'   # target_page 구할 때 이용\n",
    "\n",
    "        # 3.1. 첫 페이지 크롤링\n",
    "        current_page, data = contentCrawling()\n",
    "        if current_page == stop_page: break   # 실행종료\n",
    "        if current_page != start_page:   # 첫 페이지 찾기\n",
    "            delay('시작 페이지로 이동', 0)\n",
    "            while current_page < start_page:\n",
    "                if current_page // 10 < start_page // 10:\n",
    "                    driver.find_element_by_css_selector(page_num.format(1)).click()\n",
    "                    delay('', 2)\n",
    "                    current_page = int(driver.find_element_by_css_selector(current_page_css_selector).text)\n",
    "                    continue\n",
    "                for i in range(10, 0, -1):\n",
    "                    driver.find_element_by_css_selector(page_num.format(i)).click()\n",
    "                    delay('', 2)\n",
    "                    current_page = int(driver.find_element_by_css_selector(current_page_css_selector).text)\n",
    "                    if current_page == start_page: print('완료'); break\n",
    "        current_page, data = contentCrawling()\n",
    "        delay('', 2)\n",
    "        result.extend(data)\n",
    "        print(f'{current_page} 위치 -> {current_page} 페이지 크롤링 완료 / 누적 {len(result)} 건')\n",
    "        check_page_list.append(current_page)\n",
    "\n",
    "        # 3.2. 페이지 이동하며 크롤링\n",
    "        while True:\n",
    "            # 실행 종료\n",
    "            if current_page >= stop_page: break\n",
    "            if current_page == refresh_unit * refreshed_num: break\n",
    "\n",
    "            for i in range(10, 0, -1):\n",
    "                if current_page >= stop_page: break\n",
    "                if current_page == refresh_unit * refreshed_num: check_save_point = True; break\n",
    "\n",
    "                # 3.2.1. 크롤링할 페이지(target_page)가 현재 페이지의 다음 페이지인 지 확인\n",
    "                target_page = driver.find_element_by_css_selector(page_num.format(i))\n",
    "                if i != 1 and int(target_page.text) <= current_page: delay('', 1); continue \n",
    "\n",
    "                print(f'현재 {current_page}', end=\" -> \")\n",
    "                # 3.2.2. 크롤링할 페이지(target_page)인 다음 페이지로 이동\n",
    "                try:\n",
    "                    print(f'{target_page.text} click', end=\" -> \")\n",
    "                except Exception:\n",
    "                    driver.refresh()   # 새로고침\n",
    "                    dealy('', 5)\n",
    "                    dom = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    patent_list = dom.select('.search_section article')\n",
    "                finally:\n",
    "                    target_page.click()  # 크롤링할 페이지로 이동\n",
    "                    delay('', 2)\n",
    "                while True:\n",
    "                    try: # (클릭해서 이동한) 현재 페이지(click_page)는 target_page\n",
    "                        click_page = driver.find_element_by_css_selector(current_page_css_selector).text\n",
    "        #             except KeyboardInterrupt or WebDriverException:\n",
    "        #                 print('Interrupted')\n",
    "        #                 try: sys.exit(0)\n",
    "        #                 except SystemExit: os._exit(0)\n",
    "        #                 finally:\n",
    "        #                     print('긁어온 데이터개수 :', len(result))\n",
    "        #                     print('소요시간 :', int(time.time() - start) / 60, '분')  # 현재시각 - 시작시간 = 실행 시간\n",
    "        #                     print('프로그램 종료')\n",
    "                    except Exception:\n",
    "                        driver.refresh()   # 새로고침\n",
    "                        dom = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                        patent_list = dom.select('.search_section article')\n",
    "                        click_page = int(driver.find_element_by_css_selector(current_page_css_selector).text)\n",
    "\n",
    "                    if current_page != click_page: break  # 페이지 이동 시 loop 탈출\n",
    "                # 3.2.3. 크롤링\n",
    "                try:\n",
    "                    current_page, data = contentCrawling()\n",
    "                except Exception:\n",
    "                    driver.refresh()   # 새로고침\n",
    "                    current_page, data = contentCrawling()\n",
    "                finally:\n",
    "                    result.extend(data)\n",
    "                # 3.2.4. 중간결과 반환\n",
    "                print(f'{current_page} 크롤링 완료 / 누적 {len(result)} 건')\n",
    "                check_page_list.append(current_page)\n",
    "\n",
    "        # 4.1. 파일 저장\n",
    "        if check_save_point and check_auto_save:\n",
    "            print('=' * 60)\n",
    "            fileName = file_name.format(date=today, first_page=start_page, last_page=current_page)\n",
    "            saveFile(path=findPath(fileName), data=result, start_index=(start_page - 1) * pageSel + 1, last_index=current_page * pageSel + 1)\n",
    "\n",
    "            # 4.2. 저장 결과\n",
    "            print('예상 데이터 개수 :', (current_page - start_page + 1) * pageSel)\n",
    "            print('긁어온 데이터개수 :', len(result))\n",
    "            normal_page_list = [i for i in range(start_page, current_page + 1)]   # 정상적으로 크롤링했을 때\n",
    "            num = len(normal_page_list)\n",
    "            for i in range(start_page, current_page + 1):\n",
    "                if i in check_page_list:\n",
    "                    check_page_list.pop(check_page_list.index(i))\n",
    "                    normal_page_list.pop(normal_page_list.index(i))\n",
    "            print('중복된 페이지 :', check_page_list if check_page_list else '없음')\n",
    "            print('누락된 페이지 :', normal_page_list if normal_page_list else '없음')\n",
    "            print('=' * 60)\n",
    "            check_page_list = []\n",
    "            normal_page_list = []\n",
    "            check_save_point = False\n",
    "            result = []\n",
    "\n",
    "\n",
    "    print('=' * 60)\n",
    "    if not check_auto_save: start_page = firstPage\n",
    "    fileName = file_name.format(date=today, first_page=start_page, last_page=stop_page)\n",
    "    saveFile(path=findPath(fileName), data=result, start_index=(start_page - 1) * pageSel + 1, last_index=stop_page * pageSel + 1)\n",
    "    print('예상 데이터 개수 :', (stop_page - start_page + 1) * pageSel)\n",
    "    print('긁어온 데이터개수 :', len(result))\n",
    "    normal_page_list = [i for i in range(start_page, current_page + 1)]   # 정상적으로 크롤링했을 때\n",
    "    num = len(normal_page_list)\n",
    "    for i in range(start_page, current_page + 1):\n",
    "        if i in check_page_list:\n",
    "            check_page_list.pop(check_page_list.index(i))\n",
    "            normal_page_list.pop(normal_page_list.index(i))\n",
    "    print('중복된 페이지 :', check_page_list if check_page_list else '없음')\n",
    "    print('누락된 페이지 :', normal_page_list if normal_page_list else '없음')\n",
    "    print('소요시간 :', int(time.time() - start) / 60, '분')  # 현재시각 - 시작시간 = 실행 시간\n",
    "    print('프로그램 종료')\n",
    "    \n",
    "# [참고] schedule 모듈 : https://blog.daum.net/geoscience/1626\n",
    "# [참고] cmd pyinstaller 모듈 : https://dvlp-jun.tistory.com/26 \n",
    "# 매일 특정시간에 동작\n",
    "schedule.every().day.at(\"21:50\").do(job(start_page, refresh_unit, stop_page, check_auto_save, file_name))\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ac6f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
